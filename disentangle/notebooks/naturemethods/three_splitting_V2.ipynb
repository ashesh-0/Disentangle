{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.data_loader.train_val_data import get_train_val_data\n",
    "from disentangle.config_utils import load_config\n",
    "from disentangle.scripts.evaluate import get_data_dir\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.core.tiff_reader import save_tiff, load_tiff\n",
    "import os\n",
    "\n",
    "gt_root_dir = \"/group/jug/ashesh/kth_data\"\n",
    "pred_rootdir = \"/group/jug/ashesh/training/disentangle\"\n",
    "output_data_dir = \"/group/jug/ashesh/naturemethods/three_splitting/\"\n",
    "KTH_SAMPLE = 1\n",
    "# cropped_Input_2405_D18-M3-S0-L8_16_K1_820-1604-1800-1350\n",
    "\n",
    "\n",
    "def sample_subdir(k):\n",
    "    return \"kth{}\".format(k)\n",
    "\n",
    "\n",
    "def get_gt_dir(dtype):\n",
    "    gt_dir = os.path.join(gt_root_dir, dtype)\n",
    "    gt_dir = os.path.join(gt_dir, sample_subdir(KTH_SAMPLE))\n",
    "    return gt_dir\n",
    "\n",
    "\n",
    "def get_kth_gt(val_data):\n",
    "    if hasattr(val_data, \"_data\"):\n",
    "        gt_data = val_data._data[KTH_SAMPLE][0]\n",
    "    elif isinstance(val_data, list) and isinstance(val_data[0], tuple) and isinstance(val_data[0][1], str):\n",
    "        assert len(val_data) == 1\n",
    "        gt_data = val_data[0][0][KTH_SAMPLE]\n",
    "    else:\n",
    "        gt_data = val_data[KTH_SAMPLE]\n",
    "    return gt_data\n",
    "\n",
    "\n",
    "# /group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/pred_training_disentangle_2507_D33-M3-S0-L8_11_1.tif\n",
    "full_frame_pred_dirs = {\n",
    "    # HHMI /group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/pred_training_disentangle_2507_D34-M3-S0-L8_4_1.tif\n",
    "    # \"2507/D34-M3-S0-L8/4\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2507_D34-M3-S0-L8_4_1.tif\",\n",
    "    # \"2507/D33-M3-S0-L8/11\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2507_D33-M3-S0-L8_11_1.tif\",\n",
    "    # \"2507/D33-M3-S0-L8/25\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2507_D33-M3-S0-L8_25_1.tif\"\n",
    "    # \"2505/D32-M3-S0-L8/27\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2505_D32-M3-S0-L8_27_1.tif\",\n",
    "    # \"2507/D32-M3-S0-L8/0\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2507_D32-M3-S0-L8_0_1.tif\",\n",
    "    # \"2507/D32-M3-S0-L0/5\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2507_D32-M3-S0-L0_5_1.tif\",\n",
    "    # \"2507/D32-M3-S0-L8/7\": \"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_0/pred_training_disentangle_2507_D32-M3-S0-L8_7_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/4\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_4_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/5\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_5_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/6\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_6_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/14\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_14_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/17\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_17_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/6\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_6_1.tif\",\n",
    "    # \"2406/D25-M3-S0-L8/14\" : f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_14_1.tif',\n",
    "    # \"2406/D25-M3-S0-L8/17\": f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_17_1.tif',\n",
    "    # '2405/D25-M3-S0-L8/2': f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D25-M3-S0-L8_2_1.tif',\n",
    "    # '2405/D25-M3-S0-L8/3': f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D25-M3-S0-L8_3_1.tif',\n",
    "    \"2405/D18-M3-S0-L8/16\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D18-M3-S0-L8_16_1.tif\",\n",
    "    # '2506/D30-M3-S0-L8/6': f'/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2506_D30-M3-S0-L8_6_1.tif'\n",
    "    # '2506/D30-M3-S0-L8/5':f'/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2506_D30-M3-S0-L8_5_1.tif'\n",
    "    # '2412/D30-M3-S0-L8/0':  f'/group/jug/ashesh/data/paper_stats/Test_P64_G5-32-32_M20_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2412_D30-M3-S0-L8_0_1.tif',\n",
    "    # '2411/D30-M3-S0-L8/26': f'/group/jug/ashesh/data/paper_stats/Test_P64_G5-32-32_M20_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2411_D30-M3-S0-L8_26_1.tif',\n",
    "}\n",
    "\n",
    "\n",
    "crops_pred_dirs = {\n",
    "    # '2404/D21-M3-S0-L8/6':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D21-M3-S0-L8_6.pkl',\n",
    "    # '2404/D25-M3-S0-L8/97':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_97.pkl',\n",
    "    # '2404/D25-M3-S0-L8/120':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_120.pkl',\n",
    "    # '2404/D25-M3-S0-L8/111':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_111.pkl',\n",
    "    # '2404/D25-M3-S0-L8/125':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_125.pkl',\n",
    "    # '2404/D25-M3-S0-L8/139':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_139.pkl',\n",
    "    # '2404/D25-M3-S0-L8/143':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_143.pkl',\n",
    "    # '2405/D18-M3-S0-L8/13':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_13.pkl',\n",
    "    # '2405/D18-M3-S0-L8/14':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_14.pkl',\n",
    "    # '2405/D18-M3-S0-L8/15':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_15.pkl',\n",
    "    # '2405/D18-M3-S0-L8/10':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_10.pkl',\n",
    "    # '2405/D18-M3-S0-L8/11':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_11.pkl',\n",
    "    # '2405/D18-M3-S0-L8/12':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_12.pkl',\n",
    "    # '2404/D17-M3-S0-L8/4':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D17-M3-S0-L8_4.pkl',\n",
    "    # '2404/D21-M3-S0-L8/1':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D21-M3-S0-L8_1.pkl',\n",
    "}\n",
    "\n",
    "\n",
    "resolution_nm_dict = {\n",
    "    # '2404/D21-M3-S0-L8/6': 110,\n",
    "    # '2405/D18-M3-S0-L8/16': 27,\n",
    "    # '2412/D30-M3-S0-L8/0': 300,\n",
    "    # '2411/D30-M3-S0-L8/26': 196,\n",
    "    \"2506/D30-M3-S0-L8/5\": 196,\n",
    "    \"2506/D30-M3-S0-L8/6\": 300,\n",
    "    \"2507/D34-M3-S0-L8/4\": 45 * 4,\n",
    "    \"2507/D33-M3-S0-L8/25\": 45,\n",
    "    \"2507/D32-M3-S0-L8/0\": 45,\n",
    "    \"2507/D33-M3-S0-L8/11\": 45,\n",
    "    \"2505/D32-M3-S0-L8/27\": 45,\n",
    "    \"2507/D32-M3-S0-L0/5\": 45,\n",
    "    \"2406/D25-M3-S0-L8/4\": 285,\n",
    "    \"2406/D25-M3-S0-L8/5\": 285,\n",
    "    \"2406/D25-M3-S0-L8/6\": 285,\n",
    "    \"2406/D25-M3-S0-L8/14\": 285,\n",
    "    \"2406/D25-M3-S0-L8/17\": 285,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the kth frame data and store it for fast access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_dir, pred_fpath in full_frame_pred_dirs.items():\n",
    "    dtype = model_dir.split(\"/\")[1].split(\"-\")[0]\n",
    "    gt_dir = get_gt_dir(dtype)\n",
    "\n",
    "    if not os.path.exists(gt_dir):\n",
    "        print(\"No such dir {}. Creating it\".format(gt_dir))\n",
    "        os.makedirs(gt_dir, exist_ok=True)\n",
    "\n",
    "    gt_fpath = os.path.join(gt_dir, \"gt_for_\" + os.path.basename(pred_fpath))\n",
    "    if not os.path.exists(gt_fpath):\n",
    "        print(\"GT data is not present at {}. Creating it\".format(gt_fpath))\n",
    "        # loading directory.\n",
    "        config = load_config(os.path.join(pred_rootdir, model_dir, \"config.pkl\"))\n",
    "        val_data = get_train_val_data(\n",
    "            config.data,\n",
    "            get_data_dir(int(dtype[1:])),\n",
    "            DataSplitType.Test,\n",
    "            val_fraction=config.training.val_fraction,\n",
    "            test_fraction=config.training.test_fraction,\n",
    "        )\n",
    "        kth_gt = get_kth_gt(val_data)\n",
    "        save_tiff(gt_fpath, kth_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/group/jug/ashesh/kth_data/D18/kth1/gt_for_pred_training_disentangle_2405_D18-M3-S0-L8_16_1.tif'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_fpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = key.split('/')[1].split('-')[0]\n",
    "# config = load_config(os.path.join(pred_rootdir, key, 'config.pkl'))\n",
    "# val_data = get_train_val_data(config.data, get_data_dir(int(dtype[1:])), DataSplitType.Test,\n",
    "#         val_fraction=config.training.val_fraction,\n",
    "#         test_fraction=config.training.test_fraction)\n",
    "# kth_gt = get_kth_gt(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/localscratch/code/Disentangle/disentangle/core/tiff_reader.py:9: FutureWarning: The plugin infrastructure in `skimage.io` and the parameter `plugin` are deprecated since version 0.25 and will be removed in 0.27 (or later). To avoid this warning, please do not use the parameter `plugin`. Instead, use `imageio` or other I/O packages directly. See also `imread`.\n",
      "  data = imread(path, plugin='tifffile')\n",
      "/localscratch/code/Disentangle/disentangle/core/tiff_reader.py:9: FutureWarning: The plugin infrastructure in `skimage.io` and the parameter `plugin` are deprecated since version 0.25 and will be removed in 0.27 (or later). To avoid this warning, please do not use the parameter `plugin`. Instead, use `imageio` or other I/O packages directly. See also `imread`.\n",
      "  data = imread(path, plugin='tifffile')\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/kth_5/pred_training_disentangle_2506_D30-M3-S0-L8_5_1.tif (1024, 1024, 3) (1024, 1024, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from disentangle.core.tiff_reader import load_tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "gt_pred_dict = {}\n",
    "for model_dir, pred_fpath in tqdm(full_frame_pred_dirs.items()):\n",
    "    dtype = model_dir.split(\"/\")[1].split(\"-\")[0]\n",
    "    gt_dir = get_gt_dir(dtype)\n",
    "    gt_fpath = os.path.join(gt_dir, \"gt_for_\" + os.path.basename(pred_fpath))\n",
    "    kth_gt = load_tiff(gt_fpath)\n",
    "    kth_pred = load_tiff(pred_fpath)\n",
    "    print(pred_fpath, kth_gt.shape, kth_pred.shape)\n",
    "    gt_pred_dict[model_dir] = (kth_gt, kth_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.notion.so/Evaluating-2-channel-results-281cea8bb51c47ccadd50a389614100f?pvs=4\n",
    "\n",
    "keys = [\n",
    "    # '2405/D25-M3-S0-L8/2',\n",
    "    # '2405/D25-M3-S0-L8/3',\n",
    "    # '2405/D18-M3-S0-L8/16',\n",
    "    # \"2406/D25-M3-S0-L8/4\",\n",
    "    # \"2406/D25-M3-S0-L8/5\",\n",
    "    # \"2406/D25-M3-S0-L8/6\",\n",
    "    # \"2406/D25-M3-S0-L8/14\",\n",
    "    # \"2406/D25-M3-S0-L8/17\",\n",
    "    # '2412/D30-M3-S0-L8/0',\n",
    "    # '2411/D30-M3-S0-L8/26'\n",
    "    # \"2406/D25-M3-S0-L8/17\"\n",
    "    # '2506/D30-M3-S0-L8/6'\n",
    "    \"2506/D30-M3-S0-L8/5\",\n",
    "    # HHMI\n",
    "    # \"2507/D33-M3-S0-L8/25\"\n",
    "    # \"2507/D33-M3-S0-L8/11\"\n",
    "    # \"2507/D32-M3-S0-L8/0\",\n",
    "    # \"2505/D32-M3-S0-L8/27\"\n",
    "    # \"2507/D34-M3-S0-L8/4\"\n",
    "    # \"2507/D32-M3-S0-L0/5\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from splitting_notebooks_utils import get_input, get_config_from_saved_predictionfile_NM, get_gaussian_poisson_factors\n",
    "\n",
    "key = keys[-1]\n",
    "cfg = get_config_from_saved_predictionfile_NM(full_frame_pred_dirs[key])\n",
    "gaussian_sigma, poisson_noise_factor = get_gaussian_poisson_factors(cfg)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "\n",
    "# 5x10 grid for the input. 2x10 grid for the two targets and predictions.\n",
    "\n",
    "\n",
    "def get_cropped_img(inp, hfac, hs=None, ws=None, wN=None, extra_h=0, extra_w=0):\n",
    "    if wN is None:\n",
    "        wN = inp.shape[1]\n",
    "\n",
    "    if ws is None:\n",
    "        ws = (inp.shape[1] - wN) // 2\n",
    "    else:\n",
    "        assert ws + wN < inp.shape[1], f\"Invalid ws for the input. ws = {ws}, wN = {wN}, inp.shape = {inp.shape}\"\n",
    "\n",
    "    hN = int(wN * hfac)\n",
    "    if hs is None:\n",
    "        hs = (inp.shape[0] - hN) // 2\n",
    "    else:\n",
    "        assert hs + hN < inp.shape[0], f\"Invalid hs for the input. hs = {hs}, hN = {hN}, inp.shape = {inp.shape}\"\n",
    "\n",
    "    coords = (hs - extra_h // 2, ws - extra_w // 2, hN + extra_h // 2, wN + extra_w // 2)\n",
    "    return inp[hs - extra_h // 2 : hs + hN + extra_h // 2, ws - extra_w // 2 : ws + wN + extra_w // 2], coords\n",
    "\n",
    "\n",
    "def find_most_interesting_box(inp_crop, channel_h_factor):\n",
    "    \"\"\"\n",
    "    Returns hs, the start of the box\n",
    "    \"\"\"\n",
    "    boxH = int(np.ceil(channel_h_factor * inp_crop.shape[0]))\n",
    "    idx = np.argmax(pd.Series(inp_crop.std(axis=1)).rolling(window=boxH).mean().iloc[boxH:].values)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gt_pred_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gt_pred_dict[key][1][..., 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h_factor = 1.2\n",
    "wN = 400\n",
    "tar_wN = 300\n",
    "\n",
    "# wN = 1600\n",
    "# tar_wN = 1200\n",
    "tar_h_factor = 1.2\n",
    "\n",
    "# Target crop\n",
    "# tar_h_factor = 0.75\n",
    "tar_hN = int(tar_h_factor * tar_wN)\n",
    "\n",
    "savefig = True\n",
    "\n",
    "if savefig is False:\n",
    "    hs = np.random.randint(0, gt_pred_dict[key][0].shape[0] - int(wN * input_h_factor))\n",
    "    ws = np.random.randint(0, gt_pred_dict[key][0].shape[1] - wN)\n",
    "    print(\"ws\", ws, \"hs\", hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "# Input crop\n",
    "_, ax_inp = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "key = keys[0]\n",
    "gt, pred = gt_pred_dict[key]\n",
    "gt = gt.squeeze()\n",
    "pred = pred.squeeze()\n",
    "inp = get_input(gt, pred.shape[-1], gaussian_sigma, poisson_noise_factor)[0]\n",
    "inp_crop, input_coordinates = get_cropped_img(inp, input_h_factor, wN=wN, ws=ws, hs=hs)\n",
    "ax_inp.imshow(inp_crop, cmap=\"magma\")\n",
    "\n",
    "if key in resolution_nm_dict:\n",
    "    scalebar = ScaleBar(\n",
    "        resolution_nm_dict[key],\n",
    "        \"nm\",\n",
    "        # length_fraction=0.1,\n",
    "        box_alpha=0.6,\n",
    "        frameon=True,\n",
    "        location=\"upper right\",\n",
    "        font_properties={\"size\": 12},\n",
    "    )\n",
    "\n",
    "    ax_inp.add_artist(scalebar)\n",
    "\n",
    "if savefig:\n",
    "    ax_inp.axis(\"off\")\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Set the top-left location\n",
    "tar_rel_hs = 70\n",
    "tar_rel_ws = 50\n",
    "relative_coordinates = (tar_rel_hs, tar_rel_ws, tar_hN, tar_wN)\n",
    "assert tar_rel_hs + tar_hN < inp_crop.shape[0], (\n",
    "    f\"Invalid tar_rel_hs for the input. tar_rel_hs = {tar_rel_hs}, tar_hN = {tar_hN}, inp_crop.shape = {inp_crop.shape}\"\n",
    ")\n",
    "assert tar_rel_ws + tar_wN < inp_crop.shape[1], (\n",
    "    f\"Invalid tar_rel_ws for the input. tar_rel_ws = {tar_rel_ws}, tar_wN = {tar_wN}, inp_crop.shape = {inp_crop.shape}\"\n",
    ")\n",
    "\n",
    "rect = patches.Rectangle(\n",
    "    (relative_coordinates[1], relative_coordinates[0]),\n",
    "    relative_coordinates[3],\n",
    "    relative_coordinates[2],\n",
    "    linewidth=2,\n",
    "    edgecolor=\"w\",\n",
    "    facecolor=\"none\",\n",
    "    linestyle=\"--\",\n",
    ")\n",
    "ax_inp.add_patch(rect)\n",
    "\n",
    "if savefig:\n",
    "    # filename should contain all cropping information\n",
    "    fname = \"cropped_Input_{}_K{}_{}-{}-{}-{}.png\".format(\n",
    "        key.replace(\"/\", \"_\"),\n",
    "        KTH_SAMPLE,\n",
    "        input_coordinates[1],\n",
    "        input_coordinates[0],\n",
    "        input_coordinates[3],\n",
    "        input_coordinates[2],\n",
    "    )\n",
    "\n",
    "    fpath = os.path.join(output_data_dir, fname)\n",
    "    print(fpath)\n",
    "    plt.savefig(fpath, dpi=150, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 2\n",
    "img_sz = 3\n",
    "_, ax = plt.subplots(figsize=(ncols * img_sz, nrows * img_sz * tar_h_factor), ncols=ncols, nrows=nrows)\n",
    "\n",
    "tar_hs = tar_rel_hs + input_coordinates[0]\n",
    "tar_ws = tar_rel_ws + input_coordinates[1]\n",
    "gt_crop = gt[tar_hs : tar_hs + tar_hN, tar_ws : tar_ws + tar_wN]\n",
    "pred_crop = pred[tar_hs : tar_hs + tar_hN, tar_ws : tar_ws + tar_wN]\n",
    "for i in range(ncols):\n",
    "    vmin = np.quantile(gt_crop[..., i], 0.001)\n",
    "    vmax = np.quantile(gt_crop[..., i], 0.999)\n",
    "    ax[0, i].imshow(gt_crop[..., i], vmin=vmin, vmax=vmax, cmap=\"magma\")\n",
    "    ax[1, i].imshow(pred_crop[..., i], vmin=vmin, vmax=vmax, cmap=\"magma\")\n",
    "    ax[0, i].axis(\"off\")\n",
    "    ax[1, i].axis(\"off\")\n",
    "\n",
    "# reduce the space between the subplots\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "\n",
    "if savefig:\n",
    "    # filename should contain all cropping information\n",
    "    fname = \"cropped_TarPred_{}_K{}_{}-{}-{}-{}.png\".format(\n",
    "        key.replace(\"/\", \"_\"), KTH_SAMPLE, tar_hs, tar_ws, tar_hN, tar_wN\n",
    "    )\n",
    "    fpath = os.path.join(output_data_dir, fname)\n",
    "    print(fpath)\n",
    "    plt.savefig(fpath, dpi=150, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"cropped_TarPred_{}_K{}_{}-{}-{}-{}.png\".format(\n",
    "    key.replace(\"/\", \"_\"), KTH_SAMPLE, tar_hs, tar_ws, tar_hN, tar_wN\n",
    ")\n",
    "fpath = os.path.join(output_data_dir, fname)\n",
    "print(fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.tiff_reader import load_tiff\n",
    "\n",
    "fpath = (\n",
    "    \"/group/jug/ashesh/data/paper_stats/Test_P64_G3-32-32_M50_Sk0/pred_training_disentangle_2507_D33-M3-S0-L8_25_1.tif\"\n",
    ")\n",
    "data = load_tiff(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0, 0, 4, ..., 0])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semanticunmix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
