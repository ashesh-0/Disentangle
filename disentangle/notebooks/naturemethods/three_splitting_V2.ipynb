{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.data_loader.train_val_data import get_train_val_data\n",
    "from disentangle.config_utils import load_config\n",
    "from disentangle.scripts.evaluate import get_data_dir\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.core.tiff_reader import save_tiff, load_tiff\n",
    "import os\n",
    "\n",
    "gt_root_dir = \"/group/jug/ashesh/kth_data\"\n",
    "pred_rootdir = '/group/jug/ashesh/training/disentangle'\n",
    "output_data_dir = '/group/jug/ashesh/naturemethods/three_splitting/'\n",
    "KTH_SAMPLE = 3\n",
    "\n",
    "def sample_subdir(k):\n",
    "    return 'kth{}'.format(k)\n",
    "\n",
    "def get_gt_dir(dtype):\n",
    "    gt_dir = os.path.join(gt_root_dir, dtype)\n",
    "    gt_dir = os.path.join(gt_dir, sample_subdir(KTH_SAMPLE))\n",
    "    return gt_dir\n",
    "\n",
    "\n",
    "def get_kth_gt(val_data):\n",
    "    if hasattr(val_data, '_data'):\n",
    "        gt_data = val_data._data[KTH_SAMPLE][0]\n",
    "    elif isinstance(val_data, list) and isinstance(val_data[0], tuple) and isinstance(val_data[0][1],str):\n",
    "        assert len(val_data) == 1\n",
    "        gt_data = val_data[0][0][KTH_SAMPLE]\n",
    "    else:\n",
    "        gt_data = val_data[KTH_SAMPLE]\n",
    "    return gt_data\n",
    "\n",
    "\n",
    "\n",
    "full_frame_pred_dirs = {\n",
    "# \"2406/D25-M3-S0-L8/4\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_4_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/5\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_5_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/6\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_6_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/14\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_14_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/17\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_17_1.tif\",\n",
    "\"2505/D32-M3-S0-L8/27\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2505_D32-M3-S0-L8_27_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/6\": f\"/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_6_1.tif\",\n",
    "# \"2406/D25-M3-S0-L8/14\" : f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_14_1.tif',\n",
    "# \"2406/D25-M3-S0-L8/17\": f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2406_D25-M3-S0-L8_17_1.tif',\n",
    "# '2405/D25-M3-S0-L8/2': f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D25-M3-S0-L8_2_1.tif',\n",
    "# '2405/D25-M3-S0-L8/3': f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk8_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D25-M3-S0-L8_3_1.tif',\n",
    "# '2405/D18-M3-S0-L8/16': f'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0_F/kth_{KTH_SAMPLE}/pred_training_disentangle_2405_D18-M3-S0-L8_16_1.tif',\n",
    "# '2412/D30-M3-S0-L8/0':  f'/group/jug/ashesh/data/paper_stats/Test_P64_G5-32-32_M20_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2412_D30-M3-S0-L8_0_1.tif',\n",
    "# '2411/D30-M3-S0-L8/26': f'/group/jug/ashesh/data/paper_stats/Test_P64_G5-32-32_M20_Sk0/kth_{KTH_SAMPLE}/pred_training_disentangle_2411_D30-M3-S0-L8_26_1.tif',\n",
    "\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "crops_pred_dirs = {\n",
    "        # '2404/D21-M3-S0-L8/6':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D21-M3-S0-L8_6.pkl',\n",
    "        # '2404/D25-M3-S0-L8/97':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_97.pkl',\n",
    "        # '2404/D25-M3-S0-L8/120':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_120.pkl',\n",
    "        # '2404/D25-M3-S0-L8/111':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_111.pkl',\n",
    "        # '2404/D25-M3-S0-L8/125':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_125.pkl',\n",
    "        # '2404/D25-M3-S0-L8/139':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_139.pkl',\n",
    "        # '2404/D25-M3-S0-L8/143':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D25-M3-S0-L8_143.pkl',\n",
    "        # '2405/D18-M3-S0-L8/13':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_13.pkl',\n",
    "        # '2405/D18-M3-S0-L8/14':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_14.pkl',\n",
    "        # '2405/D18-M3-S0-L8/15':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_15.pkl',\n",
    "        # '2405/D18-M3-S0-L8/10':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_10.pkl',\n",
    "        # '2405/D18-M3-S0-L8/11':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_11.pkl',\n",
    "        # '2405/D18-M3-S0-L8/12':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2405_D18-M3-S0-L8_12.pkl',\n",
    "        # '2404/D17-M3-S0-L8/4':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D17-M3-S0-L8_4.pkl',\n",
    "        # '2404/D21-M3-S0-L8/1':'/group/jug/ashesh/data/paper_stats/Test_P64_G32_M50_Sk0/stats_disentangle_2404_D21-M3-S0-L8_1.pkl',\n",
    "}\n",
    "\n",
    "\n",
    "resolution_nm_dict = {\n",
    "    # '2404/D21-M3-S0-L8/6': 110,\n",
    "    # '2405/D18-M3-S0-L8/16': 27,\n",
    "    # '2412/D30-M3-S0-L8/0': 300,\n",
    "    # '2411/D30-M3-S0-L8/26': 196,\n",
    "    \"2505/D32-M3-S0-L8/27\": 45*2,\n",
    "    \"2406/D25-M3-S0-L8/4\": 285,\n",
    "    \"2406/D25-M3-S0-L8/5\": 285,\n",
    "    \"2406/D25-M3-S0-L8/6\": 285,\n",
    "    \"2406/D25-M3-S0-L8/14\": 285,\n",
    "    \"2406/D25-M3-S0-L8/17\": 285,\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the kth frame data and store it for fast access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model_dir, pred_fpath in full_frame_pred_dirs.items():\n",
    "    dtype = model_dir.split('/')[1].split('-')[0]\n",
    "    gt_dir = get_gt_dir(dtype)\n",
    "\n",
    "    if not os.path.exists(gt_dir):\n",
    "        print('No such dir {}. Creating it'.format(gt_dir))\n",
    "        os.makedirs(gt_dir, exist_ok=True)\n",
    "    \n",
    "    gt_fpath = os.path.join(gt_dir, 'gt_for_'+os.path.basename(pred_fpath))\n",
    "    if not os.path.exists(gt_fpath):\n",
    "        print('GT data is not present at {}. Creating it'.format(gt_fpath))\n",
    "        # loading directory.\n",
    "        config = load_config(os.path.join(pred_rootdir, model_dir, 'config.pkl'))\n",
    "        val_data = get_train_val_data(config.data, get_data_dir(int(dtype[1:])), DataSplitType.Test, \n",
    "        val_fraction=config.training.val_fraction,\n",
    "        test_fraction=config.training.test_fraction)\n",
    "        kth_gt = get_kth_gt(val_data)\n",
    "        save_tiff(gt_fpath, kth_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype = key.split('/')[1].split('-')[0]\n",
    "# config = load_config(os.path.join(pred_rootdir, key, 'config.pkl'))\n",
    "# val_data = get_train_val_data(config.data, get_data_dir(int(dtype[1:])), DataSplitType.Test, \n",
    "#         val_fraction=config.training.val_fraction,\n",
    "#         test_fraction=config.training.test_fraction)\n",
    "# kth_gt = get_kth_gt(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.tiff_reader import load_tiff\n",
    "from tqdm import tqdm\n",
    "\n",
    "gt_pred_dict = {}\n",
    "for model_dir, pred_fpath in tqdm(full_frame_pred_dirs.items()):\n",
    "    dtype = model_dir.split('/')[1].split('-')[0]\n",
    "    gt_dir = get_gt_dir(dtype)\n",
    "    gt_fpath = os.path.join(gt_dir, 'gt_for_'+os.path.basename(pred_fpath))\n",
    "    kth_gt = load_tiff(gt_fpath)\n",
    "    kth_pred = load_tiff(pred_fpath)\n",
    "    print(pred_fpath, kth_gt.shape, kth_pred.shape)\n",
    "    gt_pred_dict[model_dir] = (kth_gt, kth_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.notion.so/Evaluating-2-channel-results-281cea8bb51c47ccadd50a389614100f?pvs=4\n",
    "\n",
    "keys = [\n",
    "# '2405/D25-M3-S0-L8/2',\n",
    "# '2405/D25-M3-S0-L8/3',\n",
    "# '2405/D18-M3-S0-L8/16',\n",
    "# \"2406/D25-M3-S0-L8/4\",\n",
    "# \"2406/D25-M3-S0-L8/5\",\n",
    "# \"2406/D25-M3-S0-L8/6\",\n",
    "# \"2406/D25-M3-S0-L8/14\",\n",
    "# \"2406/D25-M3-S0-L8/17\",\n",
    "# '2412/D30-M3-S0-L8/0',\n",
    "# '2411/D30-M3-S0-L8/26'\n",
    "# \"2406/D25-M3-S0-L8/17\"\n",
    "\"2505/D32-M3-S0-L8/27\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def get_input(gt):\n",
    "    synthetic_input = None\n",
    "    if gt.shape[-1] == pred.shape[-1]:\n",
    "        inp = gt.mean(axis=-1)\n",
    "        synthetic_input = True\n",
    "    else:\n",
    "        assert gt.shape[-1] == 1 + pred.shape[-1], f'Expected {pred.shape[-1]} channels. Got {gt.shape[-1]} channels.'\n",
    "        inp = gt[...,-1]\n",
    "        synthetic_input = False\n",
    "    return inp, synthetic_input\n",
    "\n",
    "\n",
    "key = keys[-1]\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.patches as patches\n",
    "import pandas as pd\n",
    "\n",
    "# 5x10 grid for the input. 2x10 grid for the two targets and predictions.\n",
    "\n",
    "\n",
    "def get_cropped_img(inp, hfac, hs=None, ws= None, wN=None, extra_h=0, extra_w=0):\n",
    "    if wN is None:\n",
    "        wN = inp.shape[1]\n",
    "    \n",
    "    if ws is None:\n",
    "        ws = (inp.shape[1] - wN)//2\n",
    "    else:\n",
    "        assert ws + wN < inp.shape[1], f'Invalid ws for the input. ws = {ws}, wN = {wN}, inp.shape = {inp.shape}'\n",
    "    \n",
    "    hN = int(wN*hfac)\n",
    "    if hs is None:\n",
    "        hs = (inp.shape[0] - hN)//2\n",
    "    else:\n",
    "        assert hs + hN < inp.shape[0], f'Invalid hs for the input. hs = {hs}, hN = {hN}, inp.shape = {inp.shape}'\n",
    "    \n",
    "    coords = (hs-extra_h//2, ws-extra_w//2, hN+extra_h//2, wN+extra_w//2)\n",
    "    return inp[hs-extra_h//2:hs+hN+extra_h//2, ws - extra_w//2:ws+wN+extra_w//2], coords\n",
    "\n",
    "\n",
    "def find_most_interesting_box(inp_crop, channel_h_factor):\n",
    "    \"\"\"\n",
    "    Returns hs, the start of the box\n",
    "    \"\"\"\n",
    "    boxH = int(np.ceil(channel_h_factor * inp_crop.shape[0]))\n",
    "    idx = np.argmax(pd.Series(inp_crop.std(axis=1)).rolling(window=boxH).mean().iloc[boxH:].values)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gt_pred_dict[key][0][...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h_factor = 1\n",
    "wN = 500\n",
    "tar_wN = 400\n",
    "tar_h_factor = 1\n",
    "\n",
    "# Target crop\n",
    "# tar_h_factor = 0.75\n",
    "tar_hN = int(tar_h_factor*tar_wN)\n",
    "\n",
    "savefig = False\n",
    "\n",
    "if savefig is False:\n",
    "    hs = np.random.randint(0, gt_pred_dict[key][0].shape[0] -int(wN*input_h_factor))\n",
    "    ws = np.random.randint(0, gt_pred_dict[key][0].shape[1] -wN)\n",
    "    print('ws', ws, 'hs', hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hs = 500\n",
    "# ws = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "\n",
    "# Input crop\n",
    "_,ax_inp = plt.subplots(figsize=(5,5))\n",
    "\n",
    "key = keys[0]\n",
    "gt, pred = gt_pred_dict[key]\n",
    "gt = gt.squeeze()\n",
    "pred = pred.squeeze()\n",
    "inp = get_input(gt)[0]\n",
    "inp_crop, input_coordinates = get_cropped_img(inp, input_h_factor, wN=wN, ws=ws, hs=hs)\n",
    "ax_inp.imshow(inp_crop, cmap='magma')\n",
    "\n",
    "if key in resolution_nm_dict:\n",
    "    scalebar = ScaleBar(resolution_nm_dict[key], \n",
    "                        \"nm\", \n",
    "                        # length_fraction=0.1, \n",
    "                        box_alpha=0.6, frameon=True, location='upper right', font_properties={'size':12})\n",
    "\n",
    "    ax_inp.add_artist(scalebar)\n",
    "\n",
    "if savefig:\n",
    "    ax_inp.axis('off')\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Set the top-left location\n",
    "tar_rel_hs = 70\n",
    "tar_rel_ws = 50\n",
    "relative_coordinates = (tar_rel_hs,\n",
    "                        tar_rel_ws,\n",
    "                        tar_hN,\n",
    "                        tar_wN)\n",
    "assert tar_rel_hs + tar_hN < inp_crop.shape[0], f'Invalid tar_rel_hs for the input. tar_rel_hs = {tar_rel_hs}, tar_hN = {tar_hN}, inp_crop.shape = {inp_crop.shape}'\n",
    "assert tar_rel_ws + tar_wN < inp_crop.shape[1], f'Invalid tar_rel_ws for the input. tar_rel_ws = {tar_rel_ws}, tar_wN = {tar_wN}, inp_crop.shape = {inp_crop.shape}'\n",
    "\n",
    "rect = patches.Rectangle((relative_coordinates[1], relative_coordinates[0]), relative_coordinates[3],relative_coordinates[2], \n",
    "                         linewidth=2, edgecolor='w', facecolor='none', linestyle='--')\n",
    "ax_inp.add_patch(rect)\n",
    "\n",
    "if savefig:\n",
    "    # filename should contain all cropping information\n",
    "    fname = 'cropped_Input_{}_K{}_{}-{}-{}-{}.png'.format(key.replace('/','_'),KTH_SAMPLE, input_coordinates[1], input_coordinates[0], \n",
    "                                                          input_coordinates[3], input_coordinates[2])\n",
    "    \n",
    "    fpath = os.path.join(output_data_dir, fname)\n",
    "    print(fpath)\n",
    "    plt.savefig(fpath, dpi=150, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 3\n",
    "nrows = 2\n",
    "img_sz = 3\n",
    "_,ax = plt.subplots(figsize=(ncols*img_sz, nrows*img_sz*tar_h_factor), ncols=ncols, nrows=nrows)\n",
    "\n",
    "tar_hs = tar_rel_hs + input_coordinates[0]\n",
    "tar_ws = tar_rel_ws + input_coordinates[1]\n",
    "gt_crop = gt[tar_hs: tar_hs + tar_hN, tar_ws: tar_ws+ tar_wN]\n",
    "pred_crop = pred[tar_hs: tar_hs + tar_hN, tar_ws: tar_ws+ tar_wN]\n",
    "for i in range(ncols):\n",
    "    ax[0,i].imshow(gt_crop[...,i], cmap='magma')\n",
    "    ax[1,i].imshow(pred_crop[...,i], cmap='magma')\n",
    "    ax[0,i].axis('off')\n",
    "    ax[1,i].axis('off')\n",
    "\n",
    "# reduce the space between the subplots\n",
    "plt.subplots_adjust(wspace=0.01, hspace=0.01)\n",
    "\n",
    "if savefig:\n",
    "    # filename should contain all cropping information\n",
    "    fname = 'cropped_TarPred_{}_K{}_{}-{}-{}-{}.png'.format(key.replace('/','_'),KTH_SAMPLE, tar_hs, tar_ws, tar_hN, tar_wN)\n",
    "    fpath = os.path.join(output_data_dir, fname)\n",
    "    print(fpath)\n",
    "    plt.savefig(fpath, dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
