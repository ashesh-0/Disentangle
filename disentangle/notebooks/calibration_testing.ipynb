{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.metrics.calibration import Calibration\n",
    "# from disentangle.metrics.calibration import get_calibrated_factor_for_stdev\n",
    "from disentangle.analysis.paper_plots import get_first_index, get_last_index\n",
    "\n",
    "def get_calibration_stats(calibration_factors, pred, pred_std, tar_normalized):\n",
    "    scalar = calibration_factors['scalar']\n",
    "    offset = calibration_factors['offset']\n",
    "    pred_std = pred_std * scalar + offset\n",
    "    calib = Calibration(num_bins=30)\n",
    "    stats = calib.compute_stats(pred, pred_std, tar_normalized)\n",
    "    return stats\n",
    "\n",
    "# def get_calibration_factor(pred, pred_std, tar_normalized, epochs = 300, lr = 160.0, eps= 1e-8):\n",
    "#     calib_dicts = []\n",
    "#     for col_idx in range(pred.shape[-1]):\n",
    "\n",
    "#         calib_dict = get_calibrated_factor_for_stdev(pred[...,col_idx], pred_std[...,col_idx], tar_normalized[...,col_idx], \n",
    "#                                                           lr=lr, epochs=epochs)\n",
    "#         calib_dicts.append(calib_dict)\n",
    "    \n",
    "#     return calib_dicts\n",
    "\n",
    "\n",
    "def plot_calib(ax, calib_stats, ch_idx, color='b', q_s = 0.00001,q_e = 0.99999\n",
    "):\n",
    "    tmp_stats = calib_stats['calib_stats'][ch_idx]\n",
    "    tmp_rmv = tmp_stats['rmv']\n",
    "    tmp_rmse = tmp_stats['rmse']\n",
    "    count = tmp_stats['bin_count']\n",
    "    rmse_err = tmp_stats['rmse_err']\n",
    "\n",
    "    first_idx = get_first_index(count, q_s)\n",
    "    last_idx = get_last_index(count, q_e)\n",
    "    # plot the calibration curve with error bars     \n",
    "    ax.plot(tmp_rmv[first_idx:-last_idx],\n",
    "            tmp_rmse[first_idx:-last_idx],\n",
    "            '-+',\n",
    "            label='C{}'.format(ch_idx),\n",
    "            color=color,\n",
    "            )\n",
    "    rmse_floor = np.array(tmp_rmse[first_idx:-last_idx]) - np.array(rmse_err[first_idx:-last_idx])\n",
    "    rmse_ceil = np.array(tmp_rmse[first_idx:-last_idx]) + np.array(rmse_err[first_idx:-last_idx])\n",
    "    ax.fill_between(tmp_rmv[first_idx:-last_idx], rmse_floor, rmse_ceil, alpha=0.3, label='error band')\n",
    "\n",
    "    # enable the grid and set the background color to gray \n",
    "    ax.grid(True)\n",
    "    ax.set_facecolor('0.75')\n",
    "    # get x and y limits of the plot\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    minv = min(xlim[0], ylim[0])\n",
    "    maxv = max(xlim[1], ylim[1])\n",
    "    ax.plot([minv, maxv], [minv, maxv], '--', color='black')\n",
    "    print('xlim:', xlim)\n",
    "    print('ylim:', ylim)     \n",
    "\n",
    "\n",
    "def l2_fitting(pred, tar, std):\n",
    "    rmse = torch.sqrt(torch.nn.MSELoss(reduction='none')(pred, tar))\n",
    "    loss = torch.nn.MSELoss(reduction='mean')(rmse, std)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "pred = np.load('/home/ashesh.ashesh/code/Disentangle/pred_MMSE50.npy')\n",
    "pred_std = np.load('/home/ashesh.ashesh/code/Disentangle/pred_std_MMSE50.npy')\n",
    "tar_normalized = np.load('/home/ashesh.ashesh/code/Disentangle/tar_normalized_MMSE50.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_factor_dict = get_calibrated_factor_for_stdev(pred, pred_std, tar_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ch_idx = 2\n",
    "stats_dict = get_calibration_stats(calib_factor_dict[ch_idx], pred[...,ch_idx:ch_idx+1], pred_std[...,ch_idx:ch_idx+1], tar_normalized[...,ch_idx:ch_idx+1])\n",
    "_,ax = plt.subplots()\n",
    "plot_calib(ax, {'calib_stats':stats_dict}, 0, color='b', q_s = 0.00001,q_e =0.99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "q_s = 0.00001\n",
    "q_e = 0.99999\n",
    "y = stats_dict[0]['rmse']\n",
    "x = stats_dict[0]['rmv']\n",
    "count = stats_dict[0]['bin_count']\n",
    "# rmse_err = tmp_stats['rmse_err']\n",
    "\n",
    "first_idx = get_first_index(count, q_s)\n",
    "last_idx = get_last_index(count, q_e)\n",
    "x = x[first_idx:-last_idx]\n",
    "y = y[first_idx:-last_idx]\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "print('slope:', slope)\n",
    "print('intercept:', intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from disentangle.metrics.calibration import nll\n",
    "\n",
    "def get_batch_mask(bin_masks, batch_size):\n",
    "    \"\"\"\n",
    "    We get a random batch of indices from the bin_masks.\n",
    "    \"\"\"\n",
    "    b_per_bin = batch_size // len(bin_masks)\n",
    "    indices = []\n",
    "    for mask in bin_masks:\n",
    "        indices.append(np.random.choice(np.where(mask)[0], size=b_per_bin, replace=True))\n",
    "    \n",
    "    indices = np.concatenate(indices)\n",
    "    mask = np.zeros_like(bin_masks[0])\n",
    "    mask[indices] = 1\n",
    "    return mask\n",
    "\n",
    "def get_binned_masks(target, q_low=0.000001, q_high=0.999999):\n",
    "    vlow, vmax = np.quantile(target, [q_low, q_high])\n",
    "    bincount =  50\n",
    "    bins = np.linspace(vlow, vmax, bincount)\n",
    "    bin_masks = []\n",
    "    for i in range(bincount-1):\n",
    "        mask = np.logical_and(target >= bins[i], target < bins[i+1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        bin_masks.append(mask)\n",
    "    return bin_masks\n",
    "\n",
    "\n",
    "def get_calibrated_factor_for_stdev(pred, pred_std, target, batch_size=32*(512**2), epochs=500, lr=0.01, q_low=0.000001, q_high=0.999999):\n",
    "    \"\"\"\n",
    "    Here, we calibrate with multiplying the predicted std (computed from logvar) with a scalar.\n",
    "    We return the calibrated scalar. This needs to be multiplied with the std.\n",
    "    Why is the input logvar and not std? because the model typically predicts logvar and not std.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    pred = pred.reshape(-1)\n",
    "    pred_std = pred_std.reshape( -1)\n",
    "    target = target.reshape(-1)\n",
    "\n",
    "    # vlow, vmax = np.quantile(target, [q_low, q_high])\n",
    "    # bincount =  50\n",
    "    # bins = np.linspace(vlow, vmax, bincount)\n",
    "    # bin_masks = []\n",
    "    # for i in range(bincount-1):\n",
    "    #     mask = np.logical_and(target >= bins[i], target < bins[i+1])\n",
    "    #     if mask.sum() == 0:\n",
    "    #         continue\n",
    "    #     bin_masks.append(mask)\n",
    "    bin_masks = get_binned_masks(target, q_low, q_high)\n",
    "    bincount = len(bin_masks)\n",
    "    # mask = np.logical_and(target > vlow, target < vmax)\n",
    "    \n",
    "    # create a learnable scalar\n",
    "    std_scalar = torch.nn.Parameter(torch.tensor(2.0))\n",
    "    std_offset = torch.nn.Parameter(torch.tensor(0.0))\n",
    "    optimizer = torch.optim.Adam([std_scalar,std_offset], lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=50)\n",
    "    loss_arr = []\n",
    "    # tqdm with text description as loss\n",
    "    bar = tqdm(range(epochs))\n",
    "    for _ in bar:\n",
    "        optimizer.zero_grad()\n",
    "        mask = get_batch_mask(bin_masks, batch_size)\n",
    "        pred_batch = torch.Tensor(pred[mask]).cuda()\n",
    "        pred_std_batch = torch.Tensor(pred_std[mask]).cuda() * std_scalar + std_offset\n",
    "        pred_logvar_batch = 2 * torch.log(pred_std_batch)\n",
    "        target_batch = torch.Tensor(target[mask]).cuda()\n",
    "\n",
    "        # loss = torch.mean(nll(target_batch, pred_batch, pred_logvar_batch))\n",
    "        loss = l2_fitting(pred_batch, target_batch, pred_std_batch)\n",
    "        loss.backward()\n",
    "        loss_arr.append(loss.item())\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        # if learning rate is below 1e-5, break\n",
    "        if optimizer.param_groups[0]['lr'] < 1e-4:\n",
    "            break\n",
    "        bar.set_description(f'nll: {np.mean(loss_arr[-10:])} scalar: {std_scalar.item()} offset: {std_offset.item()}')\n",
    "\n",
    "    output = {'scalar':std_scalar.item(),\n",
    "                'offset':std_offset.item(), \n",
    "              'loss': loss_arr, \n",
    "              'vlow': vlow, \n",
    "              'vmax': vmax\n",
    "              }\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = get_binned_masks(tar_normalized[...,0], q_low=0.000001, q_high=0.999999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(masks))):\n",
    "    for j in range(i+1, len(masks)):\n",
    "        assert np.logical_and(masks[i], masks[j]).sum() ==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([tar_normalized[mask].mean() for mask in masks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_calibrated_factor_for_stdev(pred[...,0], pred_std[...,0], tar_normalized[...,0], lr=0.1, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_nll(tar, pred, predlogvar, bins=50):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vmin, vmax = torch.min(tar), torch.max(tar)\n",
    "    bins = torch.linspace(vmin, vmax, bins)\n",
    "    nll_vals = []\n",
    "    for i in range(bins.shape[0]-1):\n",
    "        mask = torch.logical_and(tar >= bins[i], tar < bins[i+1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        nll_vals.append(nll(tar[mask], pred[mask], predlogvar[mask]).mean().item())\n",
    "    return np.mean(nll_vals)\n",
    "\n",
    "\n",
    "def balanced_l2fitting(tar, pred, pred_std, bins=50):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    vmin, vmax = torch.min(tar), torch.max(tar)\n",
    "    bins = torch.linspace(vmin, vmax, bins)\n",
    "    loss = []\n",
    "    for i in range(bins.shape[0]-1):\n",
    "        mask = torch.logical_and(tar >= bins[i], tar < bins[i+1])\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        loss.append(l2_fitting(tar[mask], pred[mask], pred_std[mask]).mean().item())\n",
    "    return np.mean(loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 11\n",
    "offset = 0.8\n",
    "# pred_logvar = 2 * torch.log(torch.Tensor(pred_std[...,0]*factor + offset))\n",
    "# loss = balanced_nll(torch.Tensor(tar_normalized[...,0]), torch.Tensor(pred[...,0]), pred_logvar)\n",
    "loss = balanced_l2fitting(torch.Tensor(tar_normalized[...,0]), torch.Tensor(pred[...,0]), torch.Tensor(pred_std[...,0]*factor + offset))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "stats = get_calibration_stats(output, pred[...,ch_idx:ch_idx+1], pred_std[...,ch_idx:ch_idx+1], tar_normalized[...,ch_idx:ch_idx+1])\n",
    "_,ax = plt.subplots()\n",
    "plot_calib(ax, {'calib_stats':stats}, 0, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
