{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19844352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb8bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from disentangle.data_loader.patch_index_manager import TilingMode\n",
    "from disentangle.core.sampler_type import SamplerType\n",
    "from disentangle.core.loss_type import LossType\n",
    "from disentangle.data_loader.ht_iba1_ki67_rawdata_loader import SubDsetType\n",
    "from disentangle.analysis.stitch_prediction import stitch_predictions\n",
    "from disentangle.analysis.mmse_prediction import get_dset_predictions\n",
    "from disentangle.analysis.forward_operator_parameters import get_forward_operator_parameters\n",
    "from disentangle.core.psnr import PSNR\n",
    "from finetunesplit.posterior_sampler import get_transform_obj, PosteriorSampler\n",
    "from disentangle.core.psnr import RangeInvariantPsnr\n",
    "\n",
    "from finetunesplit.asymmetric_transforms import TransformEnum\n",
    "from finetunesplit.calibration.calibration_coverage import compute_for_one_batch\n",
    "from finetunesplit.calibration.grid_search import grid_search\n",
    "from finetunesplit.calibration.grid_search import plot_coverage_plot\n",
    "from finetunesplit.calibration.grid_search import get_percentage_occurance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nb_core/root_dirs.ipynb\n",
    "setup_syspath_disentangle(False)\n",
    "%run ./nb_core/disentangle_imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtype(ckpt_fpath):\n",
    "    if os.path.isdir(ckpt_fpath):\n",
    "        ckpt_fpath = ckpt_fpath[:-1] if ckpt_fpath[-1] == '/' else ckpt_fpath\n",
    "    elif os.path.isfile(ckpt_fpath):\n",
    "        ckpt_fpath = os.path.dirname(ckpt_fpath)\n",
    "    assert ckpt_fpath[-1] != '/'\n",
    "    return int(ckpt_fpath.split('/')[-2].split('-')[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237569",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "# ckpt_dir = \"/group/jug/ashesh/training/disentangle/2504/D21-M3-S0-L0/14\"\n",
    "# ckpt_dir = \"/group/jug/ashesh/training/disentangle/2504/D21-M3-S0-L0/12\"\n",
    "ckpt_dir = '/group/jug/ashesh/training/disentangle/2509/D25-M29-S0-L0/8'\n",
    "assert os.path.exists(ckpt_dir)\n",
    "\n",
    "image_size_for_grid_centers = None\n",
    "custom_image_size = None\n",
    "data_t_list = None #[0,1,2]\n",
    "tiling_mode = TilingMode.ShiftBoundary\n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "COMPUTE_LOSS = False\n",
    "use_deterministic_grid = None\n",
    "threshold = None # 0.02\n",
    "compute_kl_loss = False\n",
    "evaluate_train = False# inspect training performance\n",
    "eval_datasplit_type = DataSplitType.Test \n",
    "val_repeat_factor = None\n",
    "psnr_type = 'range_invariant' #'simple', 'range_invariant'\n",
    "\n",
    "\n",
    "\n",
    "# coverage parameters.\n",
    "# correlation preserving transform\n",
    "corr_pres_trans=True\n",
    "# enable circular padded tranlation transform\n",
    "enable_translation_transform = False\n",
    "# we oscillate around the best t with a small delta\n",
    "delta_t = 0.1\n",
    "# homography transforms.\n",
    "aug_theta_max = 0.0\n",
    "aug_theta_z_max = 0\n",
    "aug_shift_max=0.0\n",
    "enable_homography_transform = aug_theta_max > 0 or aug_theta_z_max > 0 or aug_shift_max > 0\n",
    "\n",
    "# size of the block which is used to compute the correlation\n",
    "elem_size = 10\n",
    "mmse_count = 5\n",
    "# error is computed from the first prediction\n",
    "compute_error_from_first_prediction= True\n",
    "k_forward_pass = 2\n",
    "learn_noise_characteristics = True\n",
    "\n",
    "k_prediction_mode = 'entire'\n",
    "# whether to enable the hflip, vflip and 90 degree rotation\n",
    "with_transforms = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = get_dtype(ckpt_dir)\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed5fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if with_transforms:\n",
    "    ch1_transforms = [(TransformEnum.Rotate,{}),(TransformEnum.HFlip, {}),(TransformEnum.VFlip, {})]\n",
    "    ch2_transforms = [(TransformEnum.Rotate,{}),(TransformEnum.HFlip, {}),(TransformEnum.VFlip, {})]\n",
    "else:\n",
    "    ch1_transforms = [(TransformEnum.Identity, {})]\n",
    "    ch2_transforms = [(TransformEnum.Identity, {})]\n",
    "\n",
    "if enable_translation_transform:\n",
    "    ch1_transforms.append((TransformEnum.Translate, {}))\n",
    "    ch2_transforms.append((TransformEnum.Translate, {}))\n",
    "\n",
    "if enable_homography_transform:\n",
    "    dct = {'theta_max':aug_theta_max, 'theta_z_max':aug_theta_z_max, 'shift_max':aug_shift_max, 'device': 'cuda'}\n",
    "    print('Enabling homography transform', dct)\n",
    "    ch1_transforms.append((TransformEnum.DeepInV, dct))\n",
    "    ch2_transforms.append((TransformEnum.DeepInV, dct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf2d886",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert k_prediction_mode in ['entire', 'only_transformed', 'only_first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nb_core/config_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0047fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ckpt_dir.split('/')\n",
    "idx = tokens.index('disentangle')\n",
    "if config.model.model_type == 25 and tokens[idx+1] == '2312':\n",
    "    config.model.model_type = ModelType.LadderVAERestrictedReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from disentangle.core.lowres_merge_type import LowresMergeType\n",
    "\n",
    "\n",
    "with config.unlocked():\n",
    "    if 'depth3D' in config.data and config.data.depth3D > 1:\n",
    "        config.data.mode_3D = True\n",
    "        config.model.mode_3D = True\n",
    "\n",
    "    if 'start_alpha' in config.data:\n",
    "        print('Disabling the mixing augmentation, if any')\n",
    "        config.data.start_alpha = None\n",
    "        config.data.end_alpha = None\n",
    "        config.data.alpha_weighted_target = False\n",
    "            \n",
    "    # config.model.skip_nboundary_pixels_from_loss = None\n",
    "    # if config.model.model_type == ModelType.UNet and 'n_levels' not in config.model:\n",
    "    #     config.model.n_levels = 4\n",
    "    # # if config.data.sampler_type == SamplerType.NeighborSampler:\n",
    "    #     config.data.sampler_type = SamplerType.DefaultSampler\n",
    "    #     config.loss.loss_type = LossType.Elbo\n",
    "    #     config.data.grid_size = config.data.image_size\n",
    "    # # if 'ch1_fpath_list' in config.data:\n",
    "    #     config.data.ch1_fpath_list = config.data.ch1_fpath_list[:1]\n",
    "    #     config.data.mix_fpath_list = config.data.mix_fpath_list[:1]\n",
    "    # # if config.data.data_type == DataType.Pavia2VanillaSplitting:\n",
    "    #     if 'channel_2_downscale_factor' not in config.data:\n",
    "    #         config.data.channel_2_downscale_factor = 1\n",
    "    # # if config.model.model_type == ModelType.UNet and 'init_channel_count' not in config.model:\n",
    "    #     config.model.init_channel_count = 64\n",
    "    \n",
    "    # if 'skip_receptive_field_loss_tokens' not in config.loss:\n",
    "    #     config.loss.skip_receptive_field_loss_tokens = []\n",
    "    \n",
    "    # if dtype == DataType.HTIba1Ki67:\n",
    "    #     config.data.subdset_type = SubDsetType.OnlyIba1P30\n",
    "    #     config.data.empty_patch_replacement_enabled = False\n",
    "    \n",
    "    # if 'lowres_merge_type' not in config.model.encoder:\n",
    "    #     config.model.encoder.lowres_merge_type = 0\n",
    "    # if 'validtarget_random_fraction' in config.data:\n",
    "    #     config.data.validtarget_random_fraction = None\n",
    "    \n",
    "    # if config.data.data_type == DataType.TwoDset:\n",
    "    #     config.model.model_type = ModelType.LadderVae\n",
    "    #     for key in config.data.dset1:\n",
    "    #         config.data[key] = config.data.dset1[key]\n",
    "    \n",
    "    # if 'dump_kth_frame_prediction' in config.training:\n",
    "    #     config.training.dump_kth_frame_prediction = None\n",
    "\n",
    "    # # if 'input_is_sum' not in config.data:\n",
    "    #     config.data.input_is_sum = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef646b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = config.data.data_type\n",
    "\n",
    "if dtype in [DataType.CustomSinosoid, DataType.CustomSinosoidThreeCurve]:\n",
    "    data_dir = f'{DATA_ROOT}/sinosoid_without_test/sinosoid/'\n",
    "elif dtype == DataType.OptiMEM100_014:\n",
    "    data_dir = f'{DATA_ROOT}/microscopy/'\n",
    "elif dtype == DataType.Prevedel_EMBL:\n",
    "    data_dir = f'{DATA_ROOT}/Prevedel_EMBL/PKG_3P_dualcolor_stacks/NoAverage_NoRegistration/'\n",
    "elif dtype == DataType.AllenCellMito:\n",
    "    data_dir = f'{DATA_ROOT}/allencell/2017_03_08_Struct_First_Pass_Seg/AICS-11/'\n",
    "elif dtype == DataType.SeparateTiffData:\n",
    "    data_dir = f'{DATA_ROOT}/ventura_gigascience'\n",
    "elif dtype == DataType.SemiSupBloodVesselsEMBL:\n",
    "    data_dir = f'{DATA_ROOT}/EMBL_halfsupervised/Demixing_3P'\n",
    "elif dtype == DataType.Pavia2VanillaSplitting:\n",
    "    data_dir = f'{DATA_ROOT}/pavia2'\n",
    "# elif dtype == DataType.ExpansionMicroscopyMitoTub:\n",
    "    # data_dir = f'{DATA_ROOT}/expansion_microscopy_Nick/'\n",
    "elif dtype == DataType.ShroffMitoEr:\n",
    "    data_dir = f'{DATA_ROOT}/shrofflab/'\n",
    "elif dtype == DataType.HTIba1Ki67:\n",
    "    data_dir = f'{DATA_ROOT}/Stefania/20230327_Ki67_and_Iba1_trainingdata/'\n",
    "elif dtype == DataType.BioSR_MRC:\n",
    "    data_dir = f'{DATA_ROOT}/BioSR/'\n",
    "elif dtype == DataType.ExpMicroscopyV2:\n",
    "    data_dir = f'{DATA_ROOT}/expansion_microscopy_v2/datafiles/'\n",
    "elif dtype == DataType.TavernaSox2GolgiV2:\n",
    "    data_dir = f'{DATA_ROOT}/TavernaSox2Golgi/acquisition2/'\n",
    "elif dtype == DataType.Pavia3SeqData:\n",
    "    data_dir = f'{DATA_ROOT}/pavia3_sequential_cropped/'\n",
    "elif dtype == DataType.NicolaData:\n",
    "    data_dir = f'{DATA_ROOT}/nikola_data/20240531/'\n",
    "elif dtype == DataType.Dao3ChannelWithInput:\n",
    "    data_dir = f'{DATA_ROOT}/Dao4Channel/'\n",
    "elif dtype == DataType.Dao3Channel:\n",
    "    data_dir = f'{DATA_ROOT}/Dao3Channel/'\n",
    "elif dtype == DataType.SilvioLabCSHLData:\n",
    "    data_dir = f'{DATA_ROOT}/svilen_cshl2024/'\n",
    "elif dtype == DataType.ExpMicroscopyV3:\n",
    "    data_dir = f'{DATA_ROOT}/expansion_microscopy_v4/405_NHS_488BODIPY/'\n",
    "elif dtype == DataType.Elisa3DData:\n",
    "    data_dir = f'{DATA_ROOT}/Elisa3D/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed7f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.model.noise_model_ch1_fpath = config.model.noise_model_ch1_fpath.replace('/home/ashesh.ashesh/training/', '/group/jug/ashesh/training_pre_eccv/')\n",
    "# config.model.noise_model_ch2_fpath = config.model.noise_model_ch2_fpath.replace('/home/ashesh.ashesh/training/', '/group/jug/ashesh/training_pre_eccv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nb_core/disentangle_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.multiscale_lowres_count is not None and custom_image_size is not None:\n",
    "    model.reset_for_different_output_size(custom_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "ncols = len(tar_tmp)\n",
    "nrows = 2\n",
    "_,ax = plt.subplots(figsize=(4*ncols,4*nrows),ncols=ncols,nrows=nrows)\n",
    "for i in range(min(ncols,len(inp_tmp))):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "for channel_id in range(ncols):\n",
    "    ax[1,channel_id].imshow(tar_tmp[channel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eece008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_t_list is not None:\n",
    "    val_dset.reduce_data(t_list=data_t_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e62d5d",
   "metadata": {},
   "source": [
    "### Finding the optimal mixing ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred_tiled, rec_loss, logvar_tiled, patch_psnr_tuple, pred_std_tiled, _ = get_dset_predictions(model, val_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=2,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp))\n",
    "pred = stitch_predictions(pred_tiled,val_dset )\n",
    "if 'target_idx_list' in config.data and config.data.target_idx_list is not None and len(config.data.target_idx_list) > pred[0].shape[-1]:\n",
    "    # it makes it a list. donot make it unless necessary.\n",
    "    pred = [pred[i][...,:len(config.data.target_idx_list)] for i in range(len(pred))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c94f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tar = model.data_mean['target'].cpu().numpy()\n",
    "std_tar = model.data_std['target'].cpu().numpy()\n",
    "assert mean_tar.shape == (1,2,1,1)\n",
    "assert mean_tar.shape == std_tar.shape\n",
    "\n",
    "inp_arr = []\n",
    "tar_arr = []\n",
    "for i in tqdm(range(len(val_dset))):\n",
    "    inp, tar = val_dset[i]\n",
    "    inp_arr.append(inp[None,:1])\n",
    "    tar_arr.append((tar - mean_tar)/std_tar)\n",
    "normalized_inp_patches = np.concatenate(inp_arr,axis=0)\n",
    "normalized_tar_patches = np.concatenate(tar_arr,axis=0)\n",
    "del inp_arr, tar_arr\n",
    "# inp_stitched = stitch_predictions(inp_arr, val_dset)\n",
    "# inp_stitched = [x[...,0] for x in inp_stitched]\n",
    "# tar_stitched = stitch_predictions(tar_arr, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20452adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tiled.shape, normalized_inp_patches.shape, normalized_tar_patches.shape, pred_tiled.std(), normalized_inp_patches.std(), normalized_tar_patches.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.forward_operator_parameters import get_gaussian_sigma, get_best_mixing\n",
    "tar =  stitch_predictions(normalized_tar_patches, val_dset)\n",
    "inp =  stitch_predictions(normalized_inp_patches, val_dset)\n",
    "inp = [x[...,0] for x in inp]\n",
    "\n",
    "mixing_t_tar,_ = get_best_mixing(tar, inp, plot=False)\n",
    "mixing_t_pred,_ = get_best_mixing(pred, inp, plot=False)\n",
    "\n",
    "# Now we need to find the best mu and sigma\n",
    "# estimated_inp_patches = normalized_tar_patches[:,0]*mixing_t_tar + normalized_tar_patches[:,1]*(1-mixing_t_tar)\n",
    "estimated_inp_patches = pred_tiled[:,0]*mixing_t_pred + pred_tiled[:,1]*(1-mixing_t_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b37a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_inp_patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc4891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_inp_patches[idx].mean(), normalized_inp_patches[idx,0].mean(), estimated_inp_patches[idx].std(), normalized_inp_patches[idx,0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(12,6),ncols=2)\n",
    "idx = np.random.randint(0,estimated_inp_patches.shape[0])\n",
    "ax[0].imshow(estimated_inp_patches[idx])\n",
    "ax[1].imshow(normalized_inp_patches[idx,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdev_loss(pred, target, num_patches=10, patch_size=7):\n",
    "    std_pred = 0 \n",
    "    std_target = 0\n",
    "    for i in range(num_patches):\n",
    "        idx = np.random.randint(0, pred.shape[0])\n",
    "        h = np.random.randint(0, pred.shape[1]-patch_size)\n",
    "        w = np.random.randint(0, pred.shape[2]-patch_size)\n",
    "        std_pred += pred[idx,h:h+patch_size,w:w+patch_size].std()\n",
    "        std_target += target[idx,h:h+patch_size,w:w+patch_size].std()\n",
    "    return (std_pred/num_patches - std_target/num_patches)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have an equation x = alpha * y + beta + sigma * N(0,1)\n",
    "# in pytorch create three learnable parameters, alpha, beta, sigma\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "if learn_noise_characteristics:\n",
    "    mean_inp, std_inp = val_dset.get_mean_std_for_input()\n",
    "    mean_inp = torch.Tensor(mean_inp.squeeze()).item()\n",
    "    std_inp = torch.Tensor(std_inp.squeeze()).item()\n",
    "    mean_inp, std_inp\n",
    "    def get_poisson_loss(pred_, target, mu=0, sigma=1.0):\n",
    "        # we need unnormalized data. \n",
    "        pred_ = pred_ * sigma + mu\n",
    "        target = target * sigma + mu\n",
    "\n",
    "        # print(pred_.min(), pred_.max(), target.min(), target.max())\n",
    "        # negative values do not make sense.\n",
    "        pred_ = torch.clamp(pred_, min=1e-6)\n",
    "        return torch.nn.PoissonNLLLoss(log_input=False, full=True)(pred_, target)\n",
    "    \n",
    "    def add_poisson_noise(pred_, poisson_scaling, poisson_offset, mean_inp, std_inp):\n",
    "        # relative scaling for poisson noise addition.\n",
    "        pred_ = pred_ * poisson_scaling + poisson_offset\n",
    "\n",
    "        # we need unnormalized data\n",
    "        pred_ = pred_ * std_inp + mean_inp\n",
    "        pred_ = torch.clamp(pred_, min=1e-6)\n",
    "        noisy = torch.poisson(pred_)\n",
    "        noisy = (noisy - mean_inp)/std_inp\n",
    "        noisy = noisy/poisson_scaling - poisson_offset/poisson_scaling\n",
    "        return noisy\n",
    "    \n",
    "    alpha_dyn = nn.Parameter(torch.tensor(1.0), requires_grad=True)\n",
    "    beta_dyn = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "    sigma_dyn = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "    # for poisson noise, we want another scalar and offset.\n",
    "    alpha_poisson = nn.Parameter(torch.tensor(4.0), requires_grad=True)\n",
    "    beta_poisson = nn.Parameter(torch.tensor(0.0), requires_grad=True)\n",
    "\n",
    "    batch_size = 32\n",
    "    num_steps = 10000\n",
    "    std_loss_w = 2.0\n",
    "    poisson_loss_w = 0.2\n",
    "    optimizer = torch.optim.Adamax([alpha_dyn, beta_dyn, sigma_dyn, alpha_poisson, beta_poisson ], lr=1e-3)\n",
    "    bar = tqdm(range(num_steps))\n",
    "    avg_loss = 0.0\n",
    "\n",
    "    alpha_dyn_arr = []\n",
    "    beta_dyn_arr = []\n",
    "    sigma_dyn_arr = []\n",
    "    loss_arr = []\n",
    "    std_loss_arr = []\n",
    "    poisson_loss_arr = []\n",
    "    poisson_beta_dyn_arr = []\n",
    "    poisson_alpha_dyn_arr = []\n",
    "\n",
    "    for step_idx in bar:\n",
    "        idx = np.random.choice(len(estimated_inp_patches), size=batch_size, replace=False)\n",
    "        inp_batch = torch.tensor(normalized_inp_patches[idx, 0], dtype=torch.float32, device='cuda')\n",
    "        est_inp_batch = torch.tensor(estimated_inp_patches[idx], dtype=torch.float32, device='cuda')\n",
    "        assert inp_batch.shape== est_inp_batch.shape\n",
    "        pred_inp_batch = alpha_dyn * est_inp_batch + beta_dyn \n",
    "        \n",
    "        pred_for_poisson = alpha_poisson * pred_inp_batch.detach() + beta_poisson\n",
    "        if (step_idx//500)%2 == 0:\n",
    "            poisson_loss = torch.tensor(0.0, device='cuda')\n",
    "            with_poisson_noise = add_poisson_noise(pred_inp_batch, alpha_poisson.item(), beta_poisson.item(), mean_inp, std_inp)\n",
    "        else:\n",
    "            poisson_loss = get_poisson_loss(pred_for_poisson, inp_batch, mu=mean_inp, sigma=std_inp)\n",
    "            with_poisson_noise = add_poisson_noise(pred_inp_batch, alpha_poisson, beta_poisson, mean_inp, std_inp)\n",
    "\n",
    "        gaussian_noise = sigma_dyn * torch.randn_like(est_inp_batch)\n",
    "        \n",
    "        pred_inp_batch_noisy =with_poisson_noise + gaussian_noise\n",
    "        # print(poisson_loss)\n",
    "        \n",
    "        # match the std\n",
    "        std_loss = stdev_loss(pred_inp_batch_noisy, inp_batch)\n",
    "        \n",
    "        recons_loss = ((pred_inp_batch - inp_batch)**2).mean()\n",
    "        # regularize sigma_dyn to be positive.\n",
    "        net_loss = recons_loss + 100 * torch.relu(-sigma_dyn) + std_loss_w*std_loss + poisson_loss_w * poisson_loss\n",
    "        optimizer.zero_grad()\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss = 0.9 * avg_loss + 0.1 * recons_loss.item() if avg_loss > 0 else recons_loss.item()\n",
    "        bar.set_description(f'loss: {avg_loss:.3f}, alpha_dyn: {alpha_dyn.item():.3f}, beta_dyn: {beta_dyn.item():.3f}, sigma_dyn: {sigma_dyn.item():.3f}, std_loss: {std_loss.item():.3f} poisson_loss: {poisson_loss.item():.3f} alpha_poisson: {alpha_poisson.item():.3f} beta_poisson: {beta_poisson.item():.3f}')\n",
    "        alpha_dyn_arr.append(alpha_dyn.item())\n",
    "        beta_dyn_arr.append(beta_dyn.item())\n",
    "        sigma_dyn_arr.append(sigma_dyn.item())\n",
    "        loss_arr.append(recons_loss.item())\n",
    "        std_loss_arr.append(std_loss.item()*std_loss_w)\n",
    "        poisson_loss_arr.append(poisson_loss.item()*poisson_loss_w)\n",
    "        poisson_alpha_dyn_arr.append(alpha_poisson.item())\n",
    "        poisson_beta_dyn_arr.append(beta_poisson.item())\n",
    "    print('Learned parameters: alpha, beta, sigma', alpha_dyn.item(), beta_dyn.item(), sigma_dyn.item(), alpha_poisson.item(), beta_poisson.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa5869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "if learn_noise_characteristics:\n",
    "    _,ax = plt.subplots(figsize=(20,8),ncols=5,nrows=2)\n",
    "    pd.Series(alpha_dyn_arr).rolling(100).mean().plot(ax=ax[0,0], title='alpha')\n",
    "    pd.Series(beta_dyn_arr).rolling(100).mean().plot(ax=ax[0,1], title='beta')\n",
    "    pd.Series(sigma_dyn_arr).rolling(100).mean().plot(ax=ax[0,2], title='sigma')\n",
    "    pd.Series(poisson_alpha_dyn_arr).rolling(100).mean().plot(ax=ax[0,3], title='poisson_alpha')\n",
    "    pd.Series(poisson_beta_dyn_arr).rolling(100).mean().plot(ax=ax[0,4], title='poisson_beta')\n",
    "\n",
    "\n",
    "    pd.Series(loss_arr).rolling(100).mean().plot(ax=ax[1,0], title='recons loss')\n",
    "    pd.Series(std_loss_arr).rolling(100).mean().plot(ax=ax[1,1], title='std_loss')\n",
    "    pd.Series(poisson_loss_arr).rolling(100).mean().plot(ax=ax[1,2], title='poisson_loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff0d15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.plot_utils import clean_ax\n",
    "if learn_noise_characteristics:\n",
    "    _,ax = plt.subplots(figsize=(20,15),ncols=4,nrows=3)\n",
    "    for row_idx in range(3):\n",
    "        idx = np.random.randint(0,estimated_inp_patches.shape[0])\n",
    "        print(idx)\n",
    "        ax[row_idx,0].imshow(normalized_inp_patches[idx,0])\n",
    "        ax[row_idx,1].imshow(estimated_inp_patches[idx])\n",
    "        noise_free = estimated_inp_patches[idx]*alpha_dyn.item() + beta_dyn.item()\n",
    "        gaussian_noise = sigma_dyn.item()*np.random.randn(*estimated_inp_patches[idx].shape)\n",
    "        with_poisson_noise = add_poisson_noise(torch.Tensor(noise_free), alpha_poisson.item(), beta_poisson.item(), mean_inp, std_inp).numpy()\n",
    "        ax[row_idx,2].imshow(gaussian_noise + noise_free)\n",
    "        ax[row_idx, 3].imshow(gaussian_noise + with_poisson_noise)\n",
    "        \n",
    "        if row_idx == 0:\n",
    "            ax[row_idx,0].set_title('Input')\n",
    "            ax[row_idx,1].set_title('Estimated Input')\n",
    "            ax[row_idx,2].set_title('+ gaussian noise')\n",
    "            ax[row_idx,3].set_title('+ gaussian noise + poisson noise')\n",
    "        \n",
    "    clean_ax(ax)\n",
    "    # reduce the spacing between the plots\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1b8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_tar_patches.mean(), normalized_tar_patches.std(), pred_tiled.mean(), pred_tiled.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56e5d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.forward_operator_parameters import get_gaussian_sigma, get_best_mixing\n",
    "input_gaussian_noise_std = None#get_gaussian_sigma(normalized_inp_patches)\n",
    "if learn_noise_characteristics:\n",
    "    # # I have an equation x = alpha * y + beta + sigma * N(0,1)\n",
    "    mu = beta_dyn.item()\n",
    "    sigma = alpha_dyn.item()\n",
    "    input_gaussian_noise_std = sigma_dyn.item()\n",
    "    mixing_t = mixing_t_pred\n",
    "else:\n",
    "    mixing_t, mu, sigma = get_forward_operator_parameters(val_dset, pred_tiled, normalized_inp_patches, input_gaussian_noise_std= input_gaussian_noise_std,plot=True)\n",
    "forward_operator_params = {\n",
    "    'mixing_t_min': max(0.1,mixing_t - delta_t),\n",
    "    'mixing_t_max': min(0.9,mixing_t + delta_t),\n",
    "    'mu': mu,\n",
    "    'sigma': sigma,\n",
    "    'gaussian_noise_std': input_gaussian_noise_std,\n",
    "}\n",
    "print(forward_operator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d8d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tar_st = stitch_predictions(normalized_tar_patches, val_dset)\n",
    "# inp_st = stitch_predictions(normalized_inp_patches, val_dset)\n",
    "# inp_est = [x[...,0]*mixing_t + x[...,1]*(1-mixing_t) for x in tar_st] if isinstance(tar_st, list) else tar_st[...,0]*mixing_t + tar_st[...,1]*(1-mixing_t)\n",
    "# inp_est = [x*sigma + mu for x in inp_est] if isinstance(inp_est, list) else inp_est*sigma + mu\n",
    "# # PSNR(inp_st[0][...,0], inp_est[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b7108",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTar = pred[0].shape[-1]\n",
    "is_list_prediction = isinstance(pred, list)\n",
    "tar_unnorm = (val_dset._data if not is_list_prediction else [val_dset.dsets[i]._data for i in range(len(val_dset.dsets))])\n",
    "\n",
    "if \"target_idx_list\" in config.data and config.data.target_idx_list is not None:\n",
    "    nTar =len(config.data.target_idx_list)\n",
    "\n",
    "if 'input_idx' in config.data and config.data.input_idx is not None:\n",
    "    inp_unnorm = [x[...,config.data.input_idx] for x in tar_unnorm] if is_list_prediction else tar_unnorm[...,config.data.input_idx]\n",
    "    tar_unnorm = [x[...,:nTar] for x in tar_unnorm] if is_list_prediction else tar_unnorm[...,:nTar]\n",
    "else:\n",
    "    inp_unnorm = [x.mean(axis=-1) for x in tar_unnorm]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba59f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(8,4),ncols=2)\n",
    "if is_list_prediction:\n",
    "    ax[0].imshow(pred[0][0,...,0])\n",
    "    ax[1].imshow(pred[0][0,...,1])\n",
    "else:\n",
    "    ax[0].imshow(pred[0,...,0])\n",
    "    ax[1].imshow(pred[0,...,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134763e2",
   "metadata": {},
   "source": [
    "### A model to yield augmented predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7616f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class NnModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(NnModel, self).__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)[0]\n",
    "\n",
    "singleoutput_model = NnModel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55526c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_all = get_transform_obj(ch1_transforms, ch2_transforms, correlation_preserving_transforms=corr_pres_trans)\n",
    "aug_model = PosteriorSampler(singleoutput_model, transform_all, forward_operator_params=forward_operator_params, k_predictions=mmse_count,\n",
    "                                       k_prediction_mode=k_prediction_mode, k_forward_pass=k_forward_pass,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader = DataLoader(val_dset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "for batch in dloader:\n",
    "    inp_b, tar_b = batch\n",
    "    break\n",
    "\n",
    "with torch.no_grad():\n",
    "    output, pred1_mmse = aug_model(inp_b.cuda())\n",
    "    output = [x.cpu().numpy() for x in output]\n",
    "    pred1_mmse = pred1_mmse.cpu().numpy()\n",
    "\n",
    "_,ax = plt.subplots(figsize=(6,6),ncols=2,nrows=2)\n",
    "img_idx = np.random.randint(low=0, high=len(tar_b))\n",
    "ax[0,0].set_title('Pred')\n",
    "ax[0,1].set_title('Target')\n",
    "ax[0,0].imshow(output[0][img_idx,0])\n",
    "ax[0,1].imshow(tar_b[img_idx,0])\n",
    "ax[1,0].imshow(output[0][img_idx,1])\n",
    "ax[1,1].imshow(tar_b[img_idx,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95749e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if custom_image_size is None:\n",
    "    skip_pixels = config.data.image_size - image_size_for_grid_centers\n",
    "else:\n",
    "    skip_pixels = custom_image_size - image_size_for_grid_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Looking at the sanity of LC setup.\n",
    "\n",
    "\n",
    "# _,ax = plt.subplots(figsize=(15,3),ncols=5)\n",
    "# idx = np.random.randint(len(val_dset    ))\n",
    "# print(idx)\n",
    "# inp, tar = val_dset[idx]\n",
    "# ax[0].imshow(inp[0])\n",
    "# ax[1].imshow(inp[1])\n",
    "# ax[2].imshow(inp[2])\n",
    "# ax[3].imshow(tar[0])\n",
    "# ax[4].imshow(tar[1])\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     inp_c = torch.Tensor(inp[None]).cuda()\n",
    "#     pred, _  = model(inp_c)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred2, inp_c2 = aug_model.one_forward_pass(pred, inp_c[:,1:])\n",
    "\n",
    "# _,ax = plt.subplots(figsize=(15,3),ncols=5)\n",
    "# # idx = np.random.randint(len(val_dset    ))\n",
    "# ax[0].imshow(inp_c2[0,0].cpu().numpy())\n",
    "# ax[1].imshow(inp_c2[0,1].cpu().numpy())\n",
    "# ax[2].imshow(inp_c2[0,2].cpu().numpy())\n",
    "# ax[3].imshow(pred2[0,0].cpu().numpy())\n",
    "# ax[4].imshow(pred2[0,1].cpu().numpy())\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     pred3, inp_c3 = aug_model.one_forward_pass(pred2, inp_c[:,1:])\n",
    "\n",
    "# _,ax = plt.subplots(figsize=(15,3),ncols=5)\n",
    "# # idx = np.random.randint(len(val_dset    ))\n",
    "# ax[0].imshow(inp_c3[0,0].cpu().numpy())\n",
    "# ax[1].imshow(inp_c3[0,1].cpu().numpy())\n",
    "# ax[2].imshow(inp_c3[0,2].cpu().numpy())\n",
    "# ax[3].imshow(pred3[0,0].cpu().numpy())\n",
    "# ax[4].imshow(pred3[0,1].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e043c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mmse_count = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d2f86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_b_list = []\n",
    "var_list = []\n",
    "err_list  = []\n",
    "one_step_pred_mmse_list = []\n",
    "dloader = DataLoader(val_dset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "for batch in tqdm(dloader):\n",
    "    with torch.no_grad():\n",
    "        inp_b, tar_b = batch\n",
    "        tar_b_normalized = model.normalize_target(tar_b.cuda()).cpu().numpy()\n",
    "        pred_b, one_step_pred_mmse = aug_model(inp_b.cuda())\n",
    "        \n",
    "        pred_b = [x.cpu().numpy()[:,None] for x in pred_b]\n",
    "        pred_b = np.concatenate(pred_b, axis=1)\n",
    "        one_step_pred_mmse = one_step_pred_mmse.cpu().numpy()\n",
    "        \n",
    "        \n",
    "        ec_pred_b = pred_b\n",
    "        ec_tar_b_normalized = tar_b_normalized\n",
    "        ec_one_step_pred_mmse = one_step_pred_mmse\n",
    "        if skip_pixels > 1:\n",
    "            ec_pred_b = pred_b[...,skip_pixels//2:-skip_pixels//2,skip_pixels//2:-skip_pixels//2]\n",
    "            ec_tar_b_normalized = tar_b_normalized[...,skip_pixels//2:-skip_pixels//2,skip_pixels//2:-skip_pixels//2]\n",
    "            ec_one_step_pred_mmse = one_step_pred_mmse[...,skip_pixels//2:-skip_pixels//2,skip_pixels//2:-skip_pixels//2]\n",
    "        \n",
    "        var_b, err_b = compute_for_one_batch(ec_pred_b, ec_tar_b_normalized, elem_size=elem_size,mmse_sample_for_error=ec_one_step_pred_mmse if compute_error_from_first_prediction else None,)\n",
    "        one_step_pred_mmse_list.append(one_step_pred_mmse)\n",
    "        # compute for one batch, both calibration and coverage. \n",
    "        pred_b_list.append(pred_b[:,0])\n",
    "        var_list.append(var_b)\n",
    "        err_list.append(err_b)\n",
    "\n",
    "pred_b = np.concatenate(pred_b_list, axis=0)\n",
    "var = np.concatenate(var_list, axis=0)\n",
    "err = np.concatenate(err_list, axis=0)\n",
    "one_step_pred_mmse = np.concatenate(one_step_pred_mmse_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f8df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_stitched  = stitch_predictions(pred_b, val_dset)\n",
    "pred_one_step_stitched = stitch_predictions(one_step_pred_mmse, val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06593ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(8,8),ncols=2,nrows=2)\n",
    "if is_list_prediction:\n",
    "    ax[0,0].imshow(pred_stitched[0][0,...,0])\n",
    "    ax[0,1].imshow(pred_stitched[0][0,...,1])\n",
    "    ax[1,0].imshow(pred_one_step_stitched[0][0,...,0])\n",
    "    ax[1,1].imshow(pred_one_step_stitched[0][0,...,1])\n",
    "else:\n",
    "    ax[0,0].imshow(pred_stitched[0,...,0])\n",
    "    ax[0,1].imshow(pred_stitched[0,...,1])\n",
    "    ax[1,0].imshow(pred_one_step_stitched[0,...,0])\n",
    "    ax[1,1].imshow(pred_one_step_stitched[0,...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6f1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_unnorm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aec859",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_list_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5aaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_list_prediction:\n",
    "    print(RangeInvariantPsnr(tar_unnorm[0][0,...,0]*1.0, pred_stitched[0][0,...,0]).item())\n",
    "else:\n",
    "    print(RangeInvariantPsnr(tar_unnorm[0,...,0]*1.0, pred_stitched[0,...,0]).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c2cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = []\n",
    "offsets = []\n",
    "achieved_percentiles = []\n",
    "for ch_idx in range(var.shape[1]):\n",
    "    print('Starting the grid search')\n",
    "    factor_ch, offset_ch, achieved_percentile_ch = grid_search(err[:,ch_idx], var[:,ch_idx], init_delta=10, init_factor=10,around_center=False)\n",
    "    factors.append(factor_ch)\n",
    "    offsets.append(offset_ch)\n",
    "    achieved_percentiles.append(achieved_percentile_ch)\n",
    "    \n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95265e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_,ax = plt.subplots(figsize=(7,5))\n",
    "log_scale = True\n",
    "ch_idx = 1\n",
    "sns.histplot(var[:,ch_idx,:4].reshape(-1,), bins=1000,log_scale=log_scale, ax=ax, label='Unscaled_var', stat='probability')\n",
    "sns.histplot(factors[ch_idx]*var[:,ch_idx,:4].reshape(-1,) + offsets[ch_idx], bins=1000,log_scale=log_scale, ax=ax, label='Scaled_var', stat='probability')\n",
    "sns.histplot(err[:,ch_idx], bins=1000,log_scale=log_scale, ax=ax, label='err', stat='probability')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_left_oriented = plot_coverage_plot(var, err, factors, offsets, around_center=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centered = plot_coverage_plot(var, err, factors, offsets, around_center=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc64fd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect = np.linspace(0,100,100)\n",
    "for col_idx in range(var.shape[1]):\n",
    "    act = data_centered[col_idx]['scaled'][1]\n",
    "    mean_err = np.abs(perfect - act).mean() \n",
    "    max_err = np.abs(perfect - act).max()\n",
    "    print(f'Centered: Channel {col_idx} MAE {mean_err:.2f}, MAX Err {max_err:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edb6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "perfect = np.linspace(0,100,100)\n",
    "for col_idx in range(var.shape[1]):\n",
    "    act = data_left_oriented[col_idx]['scaled'][1]\n",
    "    mean_err = np.abs(perfect - act).mean() \n",
    "    max_err = np.abs(perfect - act).max()\n",
    "    print(f'LeftOriented: Channel {col_idx} MAE {mean_err:.2f}, MAX Err {max_err:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff97d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeftOriented: Channel 0 MAE 5.19, MAX Err 12.85\n",
    "# LeftOriented: Channel 1 MAE 6.48, MAX Err 13.62\n",
    "# Centered: Channel 0 MAE 9.86, MAX Err 19.88\n",
    "# Centered: Channel 1 MAE 12.47, MAX Err 24.28\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad1c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "inp, _ = val_dset[0]\n",
    "inp = torch.Tensor(inp[None]).cuda()\n",
    "for _ in range(5):\n",
    "    out,_ = model(inp)\n",
    "    outs.append(out.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7964ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(outs[0] - outs[1]).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1799736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usplit_vdi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
