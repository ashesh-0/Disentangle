{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edd04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19844352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e7d7c01",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this notebook, we will show how to evaluate the performance of a model on a test or validation set. \n",
    "This notebook assumes that the network has already been trained and saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b7f917",
   "metadata": {},
   "source": [
    "# Input\n",
    "1. data_dir: directory of the data. The datafile should be present in the data_dir.\n",
    "2. ckpt_dir: directory of the checkpoint. The checkpoint file and config should be present in the ckpt_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import ml_collections\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from disentangle.training import create_dataset, create_model\n",
    "from disentangle.core.loss_type import LossType\n",
    "from disentangle.config_utils import load_config\n",
    "from disentangle.analysis.lvae_utils import get_img_from_forward_output\n",
    "from disentangle.analysis.plot_utils import clean_ax\n",
    "from disentangle.core.data_type import DataType\n",
    "from disentangle.core.psnr import PSNR\n",
    "from disentangle.analysis.plot_utils import get_k_largest_indices,plot_imgs_from_idx\n",
    "from disentangle.core.psnr import PSNR, RangeInvariantPsnr\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.analysis.stitch_prediction import stitch_predictions\n",
    "from disentangle.analysis.mmse_prediction import get_dset_predictions\n",
    "\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237569",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# data_dir = '/group/jug/ashesh/data/ventura_gigascience/'\n",
    "# ckpt_dir = '/group/jug/ashesh/training/disentangle/2502/D7-M3-S0-L0/2'\n",
    "\n",
    "# data_dir = '/group/jug/ashesh/data/diffsplit_HT_LIF/500ms/Ch_B-Ch_D-Ch_BD/'\n",
    "# data_dir = '/group/jug/ashesh/data/diffsplit_HT_T24/'\n",
    "# data_dir = '/group/jug/ashesh/data/diffsplit_PaviaATN/'\n",
    "data_dir  = '/group/jug/ashesh/data/diffsplit_hagen/'\n",
    "ckpt_dir = '/group/jug/ashesh/training/disentangle/2505/D33-M3-S0-L8/2'\n",
    "# data_dir = '/group/jug/ashesh/data/diffsplit_BioSR/Microtubules-vs-ER/'\n",
    "# ckpt_dir = '/group/jug/ashesh/training/disentangle/2505/D32-M3-S0-L8/3'\n",
    "\n",
    "mmse_count = 2\n",
    "image_size_for_grid_centers = 32\n",
    "custom_image_size = 64\n",
    "\n",
    "MIXING_WEIGHT = 0.5\n",
    "MAX_VAL = 1993 # This is just for HAGEN dataset !!! \n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "use_deterministic_grid = None\n",
    "threshold = None # 0.02\n",
    "compute_kl_loss = False\n",
    "evaluate_train = False# inspect training performance\n",
    "eval_datasplit_type = DataSplitType.Test\n",
    "val_repeat_factor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3360bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_checkpoint(ckpt_dir):\n",
    "    output = []\n",
    "    for filename in glob.glob(ckpt_dir + \"/*_best.ckpt\"):\n",
    "        output.append(filename)\n",
    "    assert len(output) == 1, '\\n'.join(output)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.model_type import ModelType\n",
    "config = load_config(ckpt_dir)\n",
    "config = ml_collections.ConfigDict(config)\n",
    "old_image_size = None\n",
    "with config.unlocked():\n",
    "    if 'test_fraction' not in config.training:\n",
    "        config.training.test_fraction =0.0\n",
    "        \n",
    "    if 'datadir' not in config:\n",
    "        config.datadir = ''\n",
    "    if 'encoder' not in config.model:\n",
    "        config.model.encoder = ml_collections.ConfigDict()\n",
    "        assert 'decoder' not in config.model\n",
    "        config.model.decoder = ml_collections.ConfigDict()\n",
    "    \n",
    "        config.model.encoder.dropout = config.model.dropout\n",
    "        config.model.decoder.dropout = config.model.dropout\n",
    "        config.model.encoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.decoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.encoder.n_filters = config.model.n_filters\n",
    "        config.model.decoder.n_filters = config.model.n_filters\n",
    "        \n",
    "    if 'multiscale_retain_spatial_dims' not in config.model.decoder:\n",
    "        config.model.decoder.multiscale_retain_spatial_dims = False\n",
    "        \n",
    "    if 'res_block_kernel' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_kernel = 3\n",
    "        assert 'res_block_kernel' not in config.model.decoder\n",
    "        config.model.decoder.res_block_kernel = 3\n",
    "    \n",
    "    if 'res_block_skip_padding' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_skip_padding = False\n",
    "        assert 'res_block_skip_padding' not in config.model.decoder\n",
    "        config.model.decoder.res_block_skip_padding = False\n",
    "    \n",
    "    if config.data.data_type == DataType.CustomSinosoid:\n",
    "        if 'max_vshift_factor' not in config.data:\n",
    "            config.data.max_vshift_factor = config.data.max_shift_factor\n",
    "            config.data.max_hshift_factor = 0\n",
    "        if 'encourage_non_overlap_single_channel' not in config.data:\n",
    "            config.data.encourage_non_overlap_single_channel = False\n",
    "            \n",
    "    if 'skip_bottom_layers_count' in config.model:\n",
    "        config.model.skip_bottom_layers_count = 0\n",
    "        \n",
    "    if 'logvar_lowerbound' not in config.model:\n",
    "        config.model.logvar_lowerbound = None\n",
    "    if 'train_aug_rotate' not in config.data:\n",
    "        config.data.train_aug_rotate = False\n",
    "    if 'multiscale_lowres_separate_branch' not in config.model:\n",
    "        config.model.multiscale_lowres_separate_branch = False\n",
    "    if 'multiscale_retain_spatial_dims' not in config.model:\n",
    "        config.model.multiscale_retain_spatial_dims = False\n",
    "    config.data.train_aug_rotate=False\n",
    "    \n",
    "    if 'randomized_channels' not in config.data:\n",
    "        config.data.randomized_channels = False\n",
    "        \n",
    "    if 'predict_logvar' not in config.model:\n",
    "        config.model.predict_logvar=None\n",
    "    \n",
    "    if 'batchnorm' in config.model and 'batchnorm' not in config.model.encoder:\n",
    "        assert 'batchnorm' not in config.model.decoder\n",
    "        config.model.decoder.batchnorm = config.model.batchnorm\n",
    "        config.model.encoder.batchnorm = config.model.batchnorm\n",
    "    if 'conv2d_bias' not in config.model.decoder:\n",
    "        config.model.decoder.conv2d_bias = True\n",
    "        \n",
    "    \n",
    "    if custom_image_size is not None:\n",
    "        old_image_size = config.data.image_size\n",
    "        config.data.image_size = custom_image_size\n",
    "    if image_size_for_grid_centers is not None:\n",
    "        old_grid_size = config.data.get('grid_size', \"grid_size not present\")\n",
    "        config.data.grid_size = image_size_for_grid_centers\n",
    "        config.data.val_grid_size = image_size_for_grid_centers\n",
    "\n",
    "    if use_deterministic_grid is not None:\n",
    "        config.data.deterministic_grid = use_deterministic_grid\n",
    "    if threshold is not None:\n",
    "        config.data.threshold = threshold\n",
    "    if val_repeat_factor is not None:\n",
    "        config.training.val_repeat_factor = val_repeat_factor\n",
    "    config.model.mode_pred = not compute_kl_loss\n",
    "\n",
    "    config.model.skip_nboundary_pixels_from_loss = None\n",
    "    if config.model.model_type == ModelType.UNet and 'n_levels' not in config.model:\n",
    "        config.model.n_levels = 4\n",
    "    \n",
    "    if config.model.model_type == ModelType.UNet and 'init_channel_count' not in config.model:\n",
    "        config.model.init_channel_count = 64\n",
    "    \n",
    "    if 'skip_receptive_field_loss_tokens' not in config.loss:\n",
    "        config.loss.skip_receptive_field_loss_tokens = []\n",
    "    \n",
    "    if 'lowres_merge_type' not in config.model.encoder:\n",
    "        config.model.encoder.lowres_merge_type = 0\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are doing the clipping in the other way.\n",
    "config.data.clip_percentile = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cfc106",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.data_loader.vanilla_dloader import MultiChDloader\n",
    "from disentangle.data_loader.lc_multich_dloader import LCMultiChDloader\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.data_loader.patch_index_manager import TilingMode\n",
    "\n",
    "class TiffDloaderMixed(MultiChDloader):\n",
    "    \"\"\"\n",
    "    Now, we have just the inputs, and no targets. \n",
    "    \"\"\"\n",
    "    def load_data(self, data_config, datasplit_type, val_fraction=None, test_fraction=None, allow_generation=None):\n",
    "        super().load_data(data_config, datasplit_type, val_fraction=val_fraction, test_fraction=test_fraction, allow_generation=allow_generation)\n",
    "        self._data_original = self._data\n",
    "        new_data = self._data.copy()\n",
    "        if config.data.data_type == DataType.SeparateTiffData:\n",
    "            print('Clipping the data to 1993')\n",
    "            new_data[new_data > MAX_VAL] = MAX_VAL\n",
    "\n",
    "        self._data = new_data[...,0:1] * MIXING_WEIGHT + new_data[...,1:2] * (1-MIXING_WEIGHT)\n",
    "        self._data = np.repeat(self._data, 2, axis=-1)\n",
    "        # self._data[self._data > MAX_VAL] = MAX_VAL\n",
    "\n",
    "class MultiScaleTiffDloaderMixed(LCMultiChDloader):\n",
    "    \"\"\"\n",
    "    Now, we have just the inputs, and no targets. \n",
    "    \"\"\"\n",
    "    def load_data(self, data_config, datasplit_type, val_fraction=None, test_fraction=None, allow_generation=None):\n",
    "        super().load_data(data_config, datasplit_type, val_fraction=val_fraction, test_fraction=test_fraction, allow_generation=allow_generation)\n",
    "        self._data_original = self._data\n",
    "        new_data = self._data.copy()\n",
    "        if config.data.data_type == DataType.SeparateTiffData:\n",
    "            print('Clipping the data to 1993')\n",
    "            new_data[new_data > MAX_VAL] = MAX_VAL\n",
    "\n",
    "        self._data = new_data[...,0:1] * MIXING_WEIGHT + new_data[...,1:2] * (1-MIXING_WEIGHT)\n",
    "        self._data = np.repeat(self._data, 2, axis=-1)\n",
    "        # self._data[self._data > MAX_VAL] = MAX_VAL\n",
    "\n",
    "padding_kwargs = {\n",
    "    'mode':config.data.get('padding_mode','constant'),\n",
    "}\n",
    "\n",
    "if padding_kwargs['mode'] == 'constant':\n",
    "    padding_kwargs['constant_values'] = config.data.get('padding_value',0)\n",
    "\n",
    "dloader_kwargs = {'overlapping_padding_kwargs':padding_kwargs}\n",
    "if 'multiscale_lowres_count' in config.data and config.data.multiscale_lowres_count is not None:\n",
    "    data_class = MultiScaleTiffDloaderMixed\n",
    "    dloader_kwargs['num_scales'] = config.data.multiscale_lowres_count\n",
    "    dloader_kwargs['padding_kwargs'] = padding_kwargs\n",
    "else:\n",
    "    # raise Exception('Not implemented')\n",
    "    data_class = TiffDloaderMixed\n",
    "\n",
    "if config.data.data_type in [DataType.CustomSinosoid, DataType.CustomSinosoidThreeCurve, \n",
    "                             DataType.SeparateTiffData, DataType.BioSR_MRC, DataType.TavernaSox2GolgiV2,\n",
    "                                 DataType.indiSplit_BioSR,\n",
    "                            DataType.indiSplit_HagenEtAl,\n",
    "                            DataType.indiSplit_PaviaATN,\n",
    "                            DataType.indiSplit_HTT24,\n",
    "                            DataType.indiSplit_HTLIF24,\n",
    "\n",
    "                            ]:\n",
    "    datapath = data_dir\n",
    "elif config.data.data_type == DataType.OptiMEM100_014:\n",
    "    datapath = os.path.join(data_dir, 'OptiMEM100x014.tif')\n",
    "else:\n",
    "    raise NotImplementedError(config.data.data_type)\n",
    "\n",
    "normalized_input = config.data.normalized_input\n",
    "use_one_mu_std = config.data.use_one_mu_std\n",
    "train_aug_rotate = config.data.train_aug_rotate\n",
    "enable_random_cropping = False\n",
    "# grid_alignment = TilingMode.ShiftBoundary\n",
    "print(data_class)\n",
    "\n",
    "train_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=DataSplitType.Train,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=train_aug_rotate,\n",
    "                enable_random_cropping=enable_random_cropping,\n",
    "                # grid_alignment=grid_alignment,\n",
    "                **dloader_kwargs)\n",
    "import gc\n",
    "gc.collect()\n",
    "max_val = train_dset.get_max_val()\n",
    "\n",
    "val_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=eval_datasplit_type,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=False,  # No rotation aug on validation\n",
    "                enable_random_cropping=False,\n",
    "                # No random cropping on validation. Validation is evaluated on determistic grids\n",
    "                # grid_alignment=grid_alignment,\n",
    "                max_val=max_val,\n",
    "                **dloader_kwargs\n",
    "                \n",
    "            )\n",
    "\n",
    "# For normalizing, we should be using the training data's mean and std.\n",
    "mean_val, std_val = train_dset.compute_mean_std()\n",
    "train_dset.set_mean_std(mean_val, std_val)\n",
    "val_dset.set_mean_std(mean_val, std_val)\n",
    "\n",
    "\n",
    "if evaluate_train:\n",
    "    val_dset = train_dset\n",
    "data_mean, data_std = train_dset.get_mean_std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(val_dset._data[0,...,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset._data.max(), train_dset._data.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c69e23b",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with config.unlocked():\n",
    "    if old_image_size is not None:\n",
    "        config.data.image_size = old_image_size\n",
    "    # if 'input_is_sum' not in config.data:\n",
    "    #     config.data.input_is_sum = False\n",
    "\n",
    "if config.data.target_separate_normalization is True:\n",
    "    mean_fr_model, std_fr_model = train_dset.compute_individual_mean_std()\n",
    "else:\n",
    "    mean_fr_model, std_fr_model = train_dset.get_mean_std()\n",
    "\n",
    "\n",
    "model = create_model(config, {'target':mean_fr_model},{'target':std_fr_model})\n",
    "\n",
    "ckpt_fpath = get_best_checkpoint(ckpt_dir)\n",
    "checkpoint = torch.load(ckpt_fpath)\n",
    "\n",
    "_ = model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "_= model.cuda()\n",
    "\n",
    "model.set_params_to_same_device_as(torch.Tensor(1).cuda())\n",
    "\n",
    "print('Loading from epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model has {count_parameters(model)/1000_000:.3f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.multiscale_lowres_count is not None and custom_image_size is not None:\n",
    "    model.reset_for_different_output_size(custom_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59475e42",
   "metadata": {},
   "source": [
    "## Mean/Std as present in usplit. Just run Evaluate.ipynb for this config and set it here\n",
    "Note that for indiSplit, our usecase is that input can have differet mean/std than the training data. So, we just use the mean/std computed on the input data now. the commented code is for the case when we want to use the mean/std of the training data, which yields better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.data.data_type == DataType.SeparateTiffData:\n",
    "#     mean_val_usplit = np.array([[[[237.37582]],[[237.37582]]]])\n",
    "#     std_val_usplit = np.array([[[[299.02316]],[[299.02316]]]])\n",
    "# elif config.data.data_type in [DataType.HTLIF24, DataType.BioSR_MRC, DataType.TavernaSox2GolgiV2]:\n",
    "#     mean_val_usplit, std_val_usplit = train_dset.get_mean_std()\n",
    "# else:\n",
    "#     raise NotImplementedError(config.data.data_type)\n",
    "mean_val_usplit, std_val_usplit = train_dset.get_mean_std()\n",
    "val_dset.set_mean_std(mean_val_usplit, std_val_usplit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d23d6c0",
   "metadata": {},
   "source": [
    "## Looking at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "if inp_tmp.ndim == 2:\n",
    "    inp_tmp = inp_tmp[None,...]\n",
    "\n",
    "ncols = max(len(inp_tmp),3)\n",
    "nrows = 2\n",
    "_,ax = plt.subplots(figsize=(4*ncols,4*nrows),ncols=ncols,nrows=nrows)\n",
    "for i in range(len(inp_tmp)):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "\n",
    "ax[1,0].imshow(tar_tmp[0])\n",
    "ax[1,1].imshow(tar_tmp[1])\n",
    "\n",
    "ax[0,0].set_ylabel('Input')\n",
    "ax[1,0].set_ylabel('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tiled, rec_loss, logvar, patch_psnr_tuple, prediction_std = get_dset_predictions(model, val_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=mmse_count,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b693a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = np.where(logvar.squeeze() < -6)[0]\n",
    "if len(idx_list) > 0:\n",
    "    plt.imshow(val_dset[idx_list[0]][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stitch_predictions(pred_tiled,val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ignored_pixels():\n",
    "    ignored_pixels = 1\n",
    "    while(pred[0,-ignored_pixels:,-ignored_pixels:,].std() ==0):\n",
    "        ignored_pixels+=1\n",
    "    ignored_pixels-=1\n",
    "    print(f'In {pred.shape}, last {ignored_pixels} many rows and columns are all zero.')\n",
    "    return ignored_pixels\n",
    "\n",
    "ignored_pixels_in_data = print_ignored_pixels()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8474735",
   "metadata": {},
   "source": [
    "## Ignore the pixels which are present in the last few rows and columns. \n",
    "1. They don't come in the batches. So, in prediction, they are simply zeros. So they are being are ignored right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_pixels_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadedfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_last_pixels_count = 32 if config.data.data_type == DataType.OptiMEM100_014 else ignored_pixels_in_data\n",
    "\n",
    "assert ignored_pixels_in_data <= ignore_last_pixels_count, f'Set ignore_last_pixels_count={ignored_pixels_in_data}'\n",
    "print(ignore_last_pixels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = val_dset._data_original\n",
    "if config.data.data_type == DataType.SeparateTiffData:\n",
    "    tar[ tar > MAX_VAL] = MAX_VAL\n",
    "\n",
    "def ignore_pixels(arr):\n",
    "    if ignore_last_pixels_count:\n",
    "        arr = arr[:,:-ignore_last_pixels_count,:-ignore_last_pixels_count]\n",
    "    return arr\n",
    "\n",
    "pred = ignore_pixels(pred)\n",
    "tar = ignore_pixels(tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "def _avg_psnr(target, prediction, psnr_fn):\n",
    "    output = np.mean([psnr_fn(target[i:i + 1], prediction[i:i + 1]).item() for i in range(len(prediction))])\n",
    "    return round(output, 2)\n",
    "\n",
    "\n",
    "def avg_range_inv_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, RangeInvariantPsnr)\n",
    "\n",
    "\n",
    "def avg_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, PSNR)\n",
    "\n",
    "\n",
    "def compute_masked_psnr(mask, tar1, tar2, pred1, pred2):\n",
    "    mask = mask.astype(bool)\n",
    "    mask = mask[..., 0]\n",
    "    tmp_tar1 = tar1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_pred1 = pred1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_tar2 = tar2[mask].reshape((len(tar2), -1, 1))\n",
    "    tmp_pred2 = pred2[mask].reshape((len(tar2), -1, 1))\n",
    "    psnr1 = avg_range_inv_psnr(tmp_tar1, tmp_pred1)\n",
    "    psnr2 = avg_range_inv_psnr(tmp_tar2, tmp_pred2)\n",
    "    return psnr1, psnr2\n",
    "\n",
    "def avg_ssim(target, prediction):\n",
    "    ssim = [structural_similarity(target[i],prediction[i], data_range=(target[i].max() - target[i].min())) for i in range(len(target))]\n",
    "    return np.mean(ssim),np.std(ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_mean, sep_std = model.data_mean, model.data_std\n",
    "if isinstance(sep_mean, dict):\n",
    "    sep_mean = sep_mean['target']\n",
    "    sep_std = sep_std['target']\n",
    "    \n",
    "sep_mean = sep_mean.squeeze()[None,None,None]\n",
    "sep_std = sep_std.squeeze()[None,None,None]\n",
    "\n",
    "tar_normalized = (tar - sep_mean.cpu().numpy())/sep_std.cpu().numpy()\n",
    "tar1 =tar_normalized[...,0]\n",
    "tar2 =tar_normalized[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24708c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(12,12),ncols=2,nrows=2)\n",
    "idx = np.random.randint(len(pred))\n",
    "print(idx)\n",
    "ax[0,0].imshow(pred[idx,:,:,0])\n",
    "ax[0,1].imshow(pred[idx,:,:,1])\n",
    "ax[1,0].imshow(tar1[idx,:,:])\n",
    "ax[1,1].imshow(tar2[idx,:,:])\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1, pred2 = pred[...,0].astype(np.float32), pred[...,1].astype(np.float32)\n",
    "rmse1 =np.sqrt(((pred1 - tar1)**2).reshape(len(pred1),-1).mean(axis=1))\n",
    "rmse2 =np.sqrt(((pred2 - tar2)**2).reshape(len(pred2),-1).mean(axis=1)) \n",
    "\n",
    "rmse = (rmse1 + rmse2)/2\n",
    "rmse = np.round(rmse,3)\n",
    "psnr1 = avg_psnr(tar1, pred1) \n",
    "psnr2 = avg_psnr(tar2, pred2)\n",
    "rinv_psnr1 = avg_range_inv_psnr(tar1, pred1)\n",
    "rinv_psnr2 = avg_range_inv_psnr(tar2, pred2)\n",
    "ssim1_mean, ssim1_std = avg_ssim(tar1, pred1)\n",
    "ssim2_mean, ssim2_std = avg_ssim(tar2, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87868b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'W:{MIXING_WEIGHT} {DataSplitType.name(eval_datasplit_type)}_P{custom_image_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignore_last_pixels_count}')\n",
    "print('Rec Loss',np.round(rec_loss.mean(),3) )\n",
    "print('RMSE', np.mean(rmse1).round(3), np.mean(rmse2).round(3), np.mean(rmse).round(3))\n",
    "print('[Paper] RangeInv PSNR',rinv_psnr1, rinv_psnr2 )\n",
    "print('[Paper] SSIM',round(ssim1_mean,3), round(ssim2_mean,3),'±',round((ssim1_std + ssim2_std)/2,4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658db6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sep_mean.shape, sep_std.shape)\n",
    "pred_unnormalized = pred * sep_std.cpu().numpy() + sep_mean.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_unnormalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.scripts.evaluate import compute_high_snr_stats\n",
    "\n",
    "stats_dict = compute_high_snr_stats(tar, pred_unnormalized, verbose=True)\n",
    "print(f'W:{MIXING_WEIGHT} {DataSplitType.name(eval_datasplit_type)}_P{custom_image_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignore_last_pixels_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fa6d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_str = f\"T-{MIXING_WEIGHT}_MMSE-{mmse_count}\"\n",
    "ckpt_str = 'denoiSplit_' + '_'.join(ckpt_dir.split('/')[-3:])\n",
    "result_dir = os.path.join('/group/jug/ashesh/indiSplit/', 'prediction_baselines',ckpt_str + '_' + param_str)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "print(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3dcfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "def save_tiff(path, data):\n",
    "    imsave(path, data, plugin='tifffile')\n",
    "\n",
    "\n",
    "save_tiff(os.path.join(result_dir,'pred.tif'), pred_unnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc046f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
