{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19844352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import os\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5a0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nb_core/root_dirs.ipynb\n",
    "setup_syspath_disentangle(False)\n",
    "%run nb_core/disentangle_imports.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c90081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.ssl_normalization_utils import get_input_normalized, get_input_unnormalized\n",
    "from disentangle.data_loader.patch_index_manager import TilingMode\n",
    "from disentangle.core.sampler_type import SamplerType\n",
    "from disentangle.core.loss_type import LossType\n",
    "from disentangle.data_loader.ht_iba1_ki67_rawdata_loader import SubDsetType\n",
    "from disentangle.data_loader.sox2golgi_v2_rawdata_loader import Sox2GolgiV2ChannelList\n",
    "from disentangle.data_loader.multifile_raw_dloader import SubDsetType\n",
    "import ml_collections\n",
    "from disentangle.core.tiff_reader import load_tiff\n",
    "from disentangle.analysis.paper_plots import show_for_one, get_plotoutput_dir\n",
    "from disentangle.analysis.ssl_normalization_utils import normalize_input\n",
    "from finetunesplit.asymmetric_transforms import TransformAllChannels, VFlip, Rotate, HFlip, DeepinvTransform\n",
    "import deepinv\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9748a9",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# ckpt_dir = '/group/jug/ashesh/training/disentangle/2504/D21-M3-S0-L0/14'\n",
    "ckpt_dir = '/group/jug/ashesh/training/disentangle/2504/D21-M3-S0-L0/12'\n",
    "\n",
    "# ckpt_dir = '/group/jug/ashesh/training/disentangle/2504/D21-M3-S0-L2/3'\n",
    "# ckpt_dir = \"/group/jug/ashesh/training/disentangle/2503/D17-M3-S0-L0/5\"\n",
    "# ckpt_dir = '/group/jug/ashesh/training/disentangle/2404/D21-M3-S0-L8/1'\n",
    "assert os.path.exists(ckpt_dir)\n",
    "image_size_for_grid_centers = None\n",
    "mmse_count = 2\n",
    "finetuning_image_size = 64\n",
    "evaluation_image_size = 64\n",
    "save_to_file = False\n",
    "psnr_evaluation= True\n",
    "\n",
    "\n",
    "use_selected_fpaths = None#['Test1_Slice2_a/4.nd2']\n",
    "sample_mixing_ratio = True\n",
    "\n",
    "finetuning_validation_frame_idx_list = [(0,(0,500), (0,1608))]\n",
    "finetuning_frame_idx_list = [(0,(500,1608), (0,1608))]\n",
    "# use_first_k_images = 1\n",
    "tiling_mode = TilingMode.ShiftBoundary\n",
    "real_input = True\n",
    "\n",
    "save_comparative_plots =False\n",
    "enable_calibration = False\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "COMPUTE_LOSS = False\n",
    "use_deterministic_grid = None\n",
    "threshold = None # 0.02\n",
    "compute_kl_loss = False\n",
    "evaluate_train = False# inspect training performance\n",
    "eval_datasplit_type = DataSplitType.Test \n",
    "val_repeat_factor = None\n",
    "psnr_type = 'range_invariant' #'simple', 'range_invariant'\n",
    "\n",
    "\n",
    "# augmentation related\n",
    "aug_theta_max = 0\n",
    "aug_theta_z_max = 0\n",
    "aug_shift_max=0.0\n",
    "enable_mixing_aug = False\n",
    "enable_nucleus_segmentation_based_aug = False\n",
    "nucleus_channel_idx = 1\n",
    "\n",
    "# finetuningng related \n",
    "validation_step_freq = 500\n",
    "optimally_scale_predictions = True\n",
    "use_mean_std_from_all_data = False\n",
    "max_step_count_step1 = 10_000\n",
    "max_step_count_step2 =20_000\n",
    "skip_pixels = 24\n",
    "lr_step1 = 1e-3\n",
    "lr_step2 = 1e-5\n",
    "# lookback = 10\n",
    "k_augmentations=4\n",
    "optimaization_mode = 'norm_and_network'\n",
    "k_moment_value = 2\n",
    "outputdir = '/group/jug/ashesh/finetuning_results/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab52fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "assert optimaization_mode in ['onestep', 'twostep', 'norm_and_network'], f\"Invalid optimization mode: {optimaization_mode}\"\n",
    "# for making sure this works with the other notebooks\n",
    "custom_image_size = evaluation_image_size if evaluation_image_size is not None else finetuning_image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dtype(ckpt_fpath):\n",
    "    if os.path.isdir(ckpt_fpath):\n",
    "        ckpt_fpath = ckpt_fpath[:-1] if ckpt_fpath[-1] == '/' else ckpt_fpath\n",
    "    elif os.path.isfile(ckpt_fpath):\n",
    "        ckpt_fpath = os.path.dirname(ckpt_fpath)\n",
    "    assert ckpt_fpath[-1] != '/'\n",
    "    return int(ckpt_fpath.split('/')[-2].split('-')[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7232e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = get_dtype(ckpt_dir)\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f889dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nb_core/config_loader.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0047fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_image_size is None:\n",
    "    evaluation_image_size = config.data.image_size\n",
    "if finetuning_image_size is None:\n",
    "    finetuning_image_size = config.data.image_size\n",
    "\n",
    "tokens = ckpt_dir.split('/')\n",
    "idx = tokens.index('disentangle')\n",
    "if config.model.model_type == 25 and tokens[idx+1] == '2312':\n",
    "    config.model.model_type = ModelType.LadderVAERestrictedReconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a3fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with config.unlocked():\n",
    "    if 'depth3D' in config.data and config.data.depth3D > 1:\n",
    "        config.data.mode_3D = True\n",
    "        config.model.mode_3D = True\n",
    "        \n",
    "    config.model.skip_nboundary_pixels_from_loss = None\n",
    "    if config.model.model_type == ModelType.UNet and 'n_levels' not in config.model:\n",
    "        config.model.n_levels = 4\n",
    "    if config.data.sampler_type == SamplerType.NeighborSampler:\n",
    "        config.data.sampler_type = SamplerType.DefaultSampler\n",
    "        config.loss.loss_type = LossType.Elbo\n",
    "        config.data.grid_size = config.data.image_size\n",
    "    if 'ch1_fpath_list' in config.data:\n",
    "        config.data.ch1_fpath_list = config.data.ch1_fpath_list[:1]\n",
    "        config.data.mix_fpath_list = config.data.mix_fpath_list[:1]\n",
    "    if config.data.data_type == DataType.Pavia2VanillaSplitting:\n",
    "        if 'channel_2_downscale_factor' not in config.data:\n",
    "            config.data.channel_2_downscale_factor = 1\n",
    "    if config.model.model_type == ModelType.UNet and 'init_channel_count' not in config.model:\n",
    "        config.model.init_channel_count = 64\n",
    "    \n",
    "    if 'skip_receptive_field_loss_tokens' not in config.loss:\n",
    "        config.loss.skip_receptive_field_loss_tokens = []\n",
    "    \n",
    "    if dtype == DataType.HTIba1Ki67:\n",
    "        config.data.subdset_type = SubDsetType.OnlyIba1P30\n",
    "        config.data.empty_patch_replacement_enabled = False\n",
    "    \n",
    "    if 'lowres_merge_type' not in config.model.encoder:\n",
    "        config.model.encoder.lowres_merge_type = 0\n",
    "    if 'validtarget_random_fraction' in config.data:\n",
    "        config.data.validtarget_random_fraction = None\n",
    "    \n",
    "    if config.data.data_type == DataType.TwoDset:\n",
    "        config.model.model_type = ModelType.LadderVae\n",
    "        for key in config.data.dset1:\n",
    "            config.data[key] = config.data.dset1[key]\n",
    "    \n",
    "    if 'dump_kth_frame_prediction' in config.training:\n",
    "        config.training.dump_kth_frame_prediction = None\n",
    "\n",
    "    if 'input_is_sum' not in config.data:\n",
    "        config.data.input_is_sum = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773afc2a",
   "metadata": {},
   "source": [
    "## Replace the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5f875",
   "metadata": {},
   "outputs": [],
   "source": [
    "with config.unlocked():\n",
    "    if config.data.data_type ==DataType.TavernaSox2Golgi:\n",
    "        config.data.data_type = DataType.TavernaSox2GolgiV2\n",
    "        config.data.subdset_type = SubDsetType.MultiChannel\n",
    "        if real_input:\n",
    "            print('Real Input being used')\n",
    "            # all channels: ['555-647', 'GT_Cy5', 'GT_TRITC']\n",
    "            config.data.channel_idx_list = [\n",
    "                    Sox2GolgiV2ChannelList.GT_TRITC,Sox2GolgiV2ChannelList.GT_Cy5, Sox2GolgiV2ChannelList.GT_555_647\n",
    "                ]\n",
    "            config.data.input_idx = 2\n",
    "        else:\n",
    "            print('Real Input not being used')\n",
    "            config.data.channel_idx_list = [\n",
    "                    Sox2GolgiV2ChannelList.GT_TRITC,Sox2GolgiV2ChannelList.GT_Cy5\n",
    "                ]\n",
    "            config.data.input_idx = None\n",
    "    \n",
    "        config.data.num_channels = len(config.data.channel_idx_list)\n",
    "\n",
    "    elif config.data.data_type == DataType.TavernaSox2GolgiV2:\n",
    "        if real_input:\n",
    "            print('Real Input being used')\n",
    "            if len(config.data.channel_idx_list) == 2:\n",
    "                config.data.channel_idx_list.append(Sox2GolgiV2ChannelList.GT_555_647)\n",
    "                config.data.input_idx = 2\n",
    "    \n",
    "        config.data.num_channels = len(config.data.channel_idx_list)    \n",
    "    if use_selected_fpaths is not None:\n",
    "        config.data.use_selected_fpaths = use_selected_fpaths\n",
    "    config.data.target_idx_list = [0, 1]\n",
    "    config.model.num_targets = len(config.data.target_idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f08907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if nucleus_channel_idx is not None:\n",
    "    assert config.data.channel_idx_list[nucleus_channel_idx] == Sox2GolgiV2ChannelList.GT_TRITC, f\"Invalid nucleus channel index: {nucleus_channel_idx}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef646b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = config.data.data_type\n",
    "\n",
    "if dtype in [DataType.CustomSinosoid, DataType.CustomSinosoidThreeCurve]:\n",
    "    data_dir = f'{DATA_ROOT}/sinosoid_without_test/sinosoid/'\n",
    "elif dtype == DataType.OptiMEM100_014:\n",
    "    data_dir = f'{DATA_ROOT}/microscopy/'\n",
    "elif dtype == DataType.Prevedel_EMBL:\n",
    "    data_dir = f'{DATA_ROOT}/Prevedel_EMBL/PKG_3P_dualcolor_stacks/NoAverage_NoRegistration/'\n",
    "elif dtype == DataType.AllenCellMito:\n",
    "    data_dir = f'{DATA_ROOT}/allencell/2017_03_08_Struct_First_Pass_Seg/AICS-11/'\n",
    "elif dtype == DataType.SeparateTiffData:\n",
    "    data_dir = f'{DATA_ROOT}/ventura_gigascience'\n",
    "elif dtype == DataType.SemiSupBloodVesselsEMBL:\n",
    "    data_dir = f'{DATA_ROOT}/EMBL_halfsupervised/Demixing_3P'\n",
    "elif dtype == DataType.Pavia2VanillaSplitting:\n",
    "    data_dir = f'{DATA_ROOT}/pavia2'\n",
    "# elif dtype == DataType.ExpansionMicroscopyMitoTub:\n",
    "    # data_dir = f'{DATA_ROOT}/expansion_microscopy_Nick/'\n",
    "elif dtype == DataType.ShroffMitoEr:\n",
    "    data_dir = f'{DATA_ROOT}/shrofflab/'\n",
    "elif dtype == DataType.HTIba1Ki67:\n",
    "    data_dir = f'{DATA_ROOT}/Stefania/20230327_Ki67_and_Iba1_trainingdata/'\n",
    "elif dtype == DataType.BioSR_MRC:\n",
    "    data_dir = f'{DATA_ROOT}/BioSR/'\n",
    "elif dtype == DataType.ExpMicroscopyV2:\n",
    "    data_dir = f'{DATA_ROOT}/expansion_microscopy_v2/datafiles/'\n",
    "elif dtype == DataType.TavernaSox2Golgi:\n",
    "    data_dir = f'{DATA_ROOT}/TavernaSox2Golgi/'\n",
    "elif dtype == DataType.TavernaSox2GolgiV2:\n",
    "    data_dir = f'{DATA_ROOT}/TavernaSox2Golgi/acquisition2/'\n",
    "elif dtype == DataType.Pavia3SeqData:\n",
    "    data_dir = f'{DATA_ROOT}/pavia3_sequential/'\n",
    "elif dtype == DataType.NicolaData:\n",
    "    data_dir = f'{DATA_ROOT}/nikola_data/20240531/'\n",
    "elif dtype == DataType.Dao3ChannelWithInput:\n",
    "    data_dir = f'{DATA_ROOT}/Dao4Channel/'\n",
    "elif dtype == DataType.Dao3Channel:\n",
    "    data_dir = f'{DATA_ROOT}/Dao3Channel/'\n",
    "elif dtype == DataType.SilvioLabCSHLData:\n",
    "    data_dir = f'{DATA_ROOT}/svilen_cshl2024/'\n",
    "elif dtype == DataType.ExpMicroscopyV3:\n",
    "    data_dir = f'{DATA_ROOT}/expansion_microscopy_v4/405_NHS_488BODIPY/'\n",
    "elif dtype == DataType.Elisa3DData:\n",
    "    data_dir = f'{DATA_ROOT}/Elisa3D/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edde2155",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run nb_core/disentangle_setup.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if config.data.multiscale_lowres_count is not None and custom_image_size is not None:\n",
    "#     model.reset_for_different_output_size(custom_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip_pixels is None:\n",
    "    skip_pixels = finetuning_image_size // 4\n",
    "    print(f\"skip_pixels is set to {skip_pixels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "ncols = len(tar_tmp)\n",
    "nrows = 2\n",
    "_,ax = plt.subplots(figsize=(4*ncols,4*nrows),ncols=ncols,nrows=nrows)\n",
    "for i in range(min(ncols,len(inp_tmp))):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "for channel_id in range(ncols):\n",
    "    ax[1,channel_id].imshow(tar_tmp[channel_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset.dsets[0]._data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab4946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(val_dset.dsets[0]._data[0,500:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef14dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.data_loader.dset_preloaded_data import PreloadedDset\n",
    "from copy import deepcopy\n",
    "def get_data_region(data, loc):\n",
    "    h,w = loc\n",
    "    return data[...,h[0]:h[1],w[0]:w[1],:].copy()\n",
    "    \n",
    "if hasattr(val_dset, 'dsets'):\n",
    "    ftune_data = np.concatenate([get_data_region(val_dset.dsets[loc[0]]._data, loc[1:]) for loc in finetuning_frame_idx_list], axis=0)\n",
    "    ftune_val_data = np.concatenate([get_data_region(val_dset.dsets[loc[0]]._data, loc[1:]) for loc in finetuning_validation_frame_idx_list], axis=0)\n",
    "else:\n",
    "    assert hasattr(val_dset, '_data')\n",
    "    ftune_data = np.concatenate([get_data_region(val_dset._data[loc[0]:loc[0]+1], loc[1:]) for loc in finetuning_frame_idx_list], axis=0)#val_dset._data[finetuning_frame_idx_list]\n",
    "    ftune_val_data = np.concatenate([get_data_region(val_dset._data[loc[0]:loc[0]+1], loc[1:]) for loc in finetuning_validation_frame_idx_list], axis=0)#val_dset._data[finetuning_validation_frame_idx_list]\n",
    "\n",
    "print(f\"Finetuning data shape: {ftune_data.shape}\")\n",
    "print(f\"Finetuning validation data shape: {ftune_val_data.shape}\")\n",
    "kwargs = {\n",
    "                'data_config': config.data,\n",
    "                'fpath': None,\n",
    "                 'datasplit_type': DataSplitType.Test,\n",
    "                 'val_fraction':None,\n",
    "                 'test_fraction':None,\n",
    "                 'normalized_input':config.data.normalized_input,\n",
    "                 'use_one_mu_std':config.data.use_one_mu_std,\n",
    "                 'allow_generation':False,\n",
    "                 'max_val':val_dset.get_max_val(),\n",
    "                'tiling_mode':TilingMode.ShiftBoundary,\n",
    "                #  overlapping_padding_kwargs=None,\n",
    "                #  print_vars=True\n",
    "}\n",
    "\n",
    "\n",
    "ft_dset = PreloadedDset(ftune_data, **kwargs)\n",
    "ft_val_dset = PreloadedDset(ftune_val_data, **kwargs)\n",
    "if use_mean_std_from_all_data:\n",
    "    mean_val, std_val = train_dset.compute_mean_std()\n",
    "else:\n",
    "    mean_val, std_val = ft_dset.compute_mean_std(allow_for_validation_data=True)\n",
    "print(mean_val, std_val)\n",
    "ft_dset.set_mean_std(mean_val, std_val)\n",
    "ft_val_dset.set_mean_std(mean_val, std_val)\n",
    "\n",
    "mean_dict = deepcopy(mean_val)\n",
    "std_dict = deepcopy(std_val)\n",
    "model = create_model(config, mean_dict,std_dict)\n",
    "print('Loading checkpoint from', ckpt_fpath)\n",
    "checkpoint = torch.load(ckpt_fpath)\n",
    "_ = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "_= model.cuda()\n",
    "model.set_params_to_same_device_as(torch.Tensor(1).cuda())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77918a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert save_comparative_plots is False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217aae0",
   "metadata": {},
   "source": [
    "## Self-supervision\n",
    "Here, we now do some self-supervised finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.stitch_prediction import stitch_predictions\n",
    "from disentangle.analysis.mmse_prediction import get_dset_predictions\n",
    "from deepinv.transform.projective import Homography\n",
    "from finetunesplit.asymmetric_transforms import get_inverse_transforms\n",
    "from disentangle.core.psnr import RangeInvariantPsnr\n",
    "def set_image_size_for_evaluation():\n",
    "      ft_dset.set_img_sz(evaluation_image_size, evaluation_image_size//2 )\n",
    "      ft_val_dset.set_img_sz(evaluation_image_size, evaluation_image_size//2)\n",
    "      # model.reset_for_different_output_size(evaluation_image_size)\n",
    "\n",
    "def set_image_size_for_finetuning():\n",
    "    ft_dset.set_img_sz(finetuning_image_size, finetuning_image_size//2)\n",
    "    ft_val_dset.set_img_sz(evaluation_image_size, evaluation_image_size//2)\n",
    "    model.reset_for_different_output_size(finetuning_image_size)\n",
    "\n",
    "ft_dset.eval_mode()\n",
    "set_image_size_for_evaluation()\n",
    "print('Dset shape', ft_dset[0][0].shape, ft_dset[0][1].shape, ft_val_dset[0][0].shape, ft_val_dset[0][1].shape)\n",
    "pred_tiled, rec_loss, logvar_tiled, patch_psnr_tuple, pred_std_tiled, inp_patches = get_dset_predictions(model, ft_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=mmse_count,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                                return_input=True,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6ef5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_channels = np.mean(pred_tiled, axis=(2,3)).mean(axis=0)\n",
    "std_channels = np.std(pred_tiled, axis=(2,3)).mean(axis=0)\n",
    "mean_channels, std_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647410cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.loss.ssl_finetuning import get_stats_loss_func\n",
    "stats_loss_func = get_stats_loss_func(pred_tiled, k_moment_value)\n",
    "stats_loss_func(torch.Tensor(pred_tiled[:15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1573f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ft_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ac350",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.quantile(rec_loss, [0,0.01,0.5, 0.9,0.99,0.999,1]).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_tiled.shape[-1] != ft_dset.get_img_sz():\n",
    "    pad = (ft_dset.get_img_sz() - pred_tiled.shape[-1] )//2\n",
    "    pred_tiled = np.pad(pred_tiled, ((0,0),(0,0),(pad,pad),(pad,pad)))\n",
    "\n",
    "pred = stitch_predictions(pred_tiled,ft_dset, )\n",
    "inp = stitch_predictions(inp_patches,ft_dset, )\n",
    "if len(np.unique(logvar_tiled)) == 1:\n",
    "    logvar = None\n",
    "else:\n",
    "    logvar = stitch_predictions(logvar_tiled,ft_dset, )\n",
    "pred_std = stitch_predictions(pred_std_tiled,ft_dset, )\n",
    "\n",
    "pred_original = pred.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123b50c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_list_prediction = isinstance(pred, list)\n",
    "tar_unnorm = (ft_dset._data if not is_list_prediction else [ft_dset.dsets[i]._data for i in range(len(ft_dset.dsets))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67288f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_unnorm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d673fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_list_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66deb75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTar = pred[0].shape[-1]\n",
    "if \"target_idx_list\" in config.data and config.data.target_idx_list is not None:\n",
    "    nTar =len(config.data.target_idx_list)\n",
    "    # pred = pred[..., :len(config.data.target_idx_list)] if not is_list_prediction else [pred[i][..., :len(config.data.target_idx_list)] for i in range(len(pred))]\n",
    "    # pred_std = pred_std[...,:len(config.data.target_idx_list)] if not is_list_prediction else [pred_std[i][..., :len(config.data.target_idx_list)] for i in range(len(pred_std))]\n",
    "\n",
    "if 'input_idx' in config.data and config.data.input_idx is not None:\n",
    "    inp_unnorm = [x[...,config.data.input_idx] for x in tar_unnorm]\n",
    "    tar_unnorm = [x[...,:nTar] for x in tar_unnorm]\n",
    "else:\n",
    "    inp_unnorm = [x.mean(axis=-1) for x in tar_unnorm] if is_list_prediction else tar_unnorm.mean(axis=-1)\n",
    "\n",
    "actual_input = normalize_input(inp_unnorm, ft_dset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3602c8",
   "metadata": {},
   "source": [
    "### Optimal Estimation of t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "mean_psnr_arr = []\n",
    "std_psnr_arr = []\n",
    "t_values = np.arange(0.0,1.0, 0.05) \n",
    "for t in tqdm(t_values):\n",
    "    inp_tiled = [(t *pred[i][...,0] + (1-t)*pred[i][...,1]) for i in range(len(pred))]\n",
    "    psnr_values = [RangeInvariantPsnr(inp_unnorm[i]*1.0, inp_tiled[i]).item() for i in range(len(inp_unnorm))]\n",
    "    mean_psnr_arr.append(np.mean(psnr_values))\n",
    "    std_psnr_arr.append(np.std(psnr_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e04e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmax(mean_psnr_arr)\n",
    "best_t_estimate = t_values[best_idx]\n",
    "print(f'Best t value: {best_t_estimate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_values, mean_psnr_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = stitch_predictions(inp_patches, ft_dset)\n",
    "recons_inp = [(best_t_estimate *pred[i][...,0] + (1-best_t_estimate)*pred[i][...,1]) for i in range(len(pred))]\n",
    "\n",
    "def get_optimal_scalar_transformation(y, x):\n",
    "    \"\"\"\n",
    "    We want to scale the input x to match the target y.\n",
    "    y <==> factor*x + offset\n",
    "    \"\"\"\n",
    "    y = np.array(y).flatten()\n",
    "    x = np.array(x).flatten()\n",
    "    u_y = np.mean(y)\n",
    "    u_x = np.mean(x)\n",
    "\n",
    "    sigma = np.sum((x-u_x)*(x-u_x))/(np.sum((y-u_y)*(x-u_x)))\n",
    "    offset = u_y  - u_x/sigma\n",
    "    return 1/sigma, offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_factor, input_offset = get_optimal_scalar_transformation(inp, recons_inp)\n",
    "print('Input factor:', input_factor, 'Input offset:', input_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.psnr import PSNR\n",
    "print(PSNR(inp[0].squeeze()[None], recons_inp[0].squeeze()[None]))\n",
    "print(PSNR(inp[0].squeeze()[None], recons_inp[0].squeeze()[None]*input_factor + input_offset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ignored_pixels():\n",
    "    ignored_pixels = 1\n",
    "    while(pred[0,-ignored_pixels:,-ignored_pixels:,].std() ==0):\n",
    "        ignored_pixels+=1\n",
    "    ignored_pixels-=1\n",
    "    print(f'In {pred.shape}, last {ignored_pixels} many rows and columns are all zero.')\n",
    "    return ignored_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23db3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_mean, sep_std = model.data_mean[\"target\"], model.data_std[\"target\"]\n",
    "sep_mean = sep_mean.squeeze().reshape(1, 1, 1, -1)\n",
    "sep_std = sep_std.squeeze().reshape(1, 1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60d09f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0].shape, sep_mean.shape, sep_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd0084",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ft_dset._data[0,...,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa912f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unnorm_pred(pred_arr):\n",
    "    if is_list_prediction:\n",
    "        pred_unnorm = [pred_arr[i] * sep_std.cpu().numpy() + sep_mean.cpu().numpy() for i in range(len(pred_arr))]\n",
    "    else:\n",
    "        pred_unnorm = pred_arr * sep_std.cpu().numpy() + sep_mean.cpu().numpy()\n",
    "    return pred_unnorm\n",
    "\n",
    "def normalize_channels(channel_arr):\n",
    "    if is_list_prediction:\n",
    "        pred_unnorm = [(channel_arr[i] - sep_mean.cpu().numpy())/sep_std.cpu().numpy() for i in range(len(channel_arr))]\n",
    "    else:\n",
    "        pred_unnorm = (channel_arr - sep_mean.cpu().numpy())/ sep_std.cpu().numpy()\n",
    "    return pred_unnorm\n",
    "\n",
    "pred_unnorm = get_unnorm_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac44cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.scripts.evaluate import compute_high_snr_stats\n",
    "stats_dict = compute_high_snr_stats(tar_unnorm, pred_unnorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4b7b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.scripts.evaluate import avg_range_inv_psnr\n",
    "\n",
    "def get_psnr_numbers(tar_unnormalized, pred_unnormalized):\n",
    "    tar0 = [x[...,0].squeeze() for x in tar_unnormalized]\n",
    "    pred0 = [x[...,0].squeeze() for x in pred_unnormalized]\n",
    "\n",
    "    tar1 = [x[...,1].squeeze() for x in tar_unnormalized]\n",
    "    pred1 = [x[...,1].squeeze() for x in pred_unnormalized]\n",
    "\n",
    "    # check that their shape are same, and are 2D \n",
    "    assert all([tar0[i].shape == pred0[i].shape for i in range(len(tar0))])\n",
    "    assert all([tar1[i].shape == pred1[i].shape for i in range(len(tar1))])\n",
    "    assert tar0[0].ndim == 2\n",
    "    assert tar1[0].ndim == 2\n",
    "\n",
    "    psnr_ch0 = avg_range_inv_psnr(tar0, pred0)\n",
    "    psnr_ch1 = avg_range_inv_psnr(tar1, pred1)\n",
    "    return psnr_ch0, psnr_ch1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764818c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_input = get_input_normalized(pred_original,best_t_estimate, channel_pos='last')\n",
    "print('MSE-input', ' '.join([f'{np.mean((gt_tmp - pred_tmp)**2):.2f}' for gt_tmp,pred_tmp in zip(actual_input, estimated_input)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_inp, std_inp = ft_dset.dsets[0].get_mean_std_for_input() if hasattr(ft_dset, 'dsets') else ft_dset.get_mean_std_for_input()\n",
    "mean_inp = mean_inp.reshape(-1,)[0]\n",
    "std_inp = std_inp.reshape(-1,)[0]\n",
    "(mean_inp, std_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cb2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.ssl_plots import plot_one_sample\n",
    "img_idx, hs, ws = plot_one_sample(normalize_channels(tar_unnorm), pred, inp_unnorm, config, best_t_estimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba25fd8",
   "metadata": {},
   "source": [
    "## SSL finetuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b052ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from finetunesplit.loss import SSL_loss\n",
    "from disentangle.loss.ssl_finetuning import finetune_two_forward_passes\n",
    "from finetunesplit.asymmetric_transforms import TransformAllChannels, VFlip, Rotate, HFlip, DeepinvTransform\n",
    "from deepinv.transform.projective import Homography\n",
    "\n",
    "\n",
    "def reload():\n",
    "    print('Loading checkpoint from', ckpt_fpath)\n",
    "    checkpoint = torch.load(ckpt_fpath)\n",
    "    _ = model.load_state_dict(checkpoint['state_dict'], strict=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ad6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 128\n",
    "# validation_step_freq = 1000\n",
    "# max_step_count_step1 = 2000\n",
    "# psnr_evaluation= True\n",
    "# reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b0172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetunesplit.segmentation_shuffle import SegmentationShuffle, get_segmentation_fn\n",
    "set_image_size_for_finetuning()\n",
    "\n",
    "transform_types = {0:[VFlip(), Rotate(),HFlip()], 1:[ VFlip(), HFlip(), Rotate()]}\n",
    "if aug_shift_max > 0 or aug_theta_z_max or aug_theta_max > 0:\n",
    "    print('Applying homography')\n",
    "    trans_homo = Homography(n_trans = 1, zoom_factor_min=1.0, theta_max=aug_theta_max, \n",
    "                            theta_z_max=aug_theta_z_max, \n",
    "                            skew_max=0, \n",
    "                            shift_max=aug_shift_max,\n",
    "                            x_stretch_factor_min = 1,\n",
    "                            y_stretch_factor_min = 1, device = model.device)\n",
    "    transform_types[0].append(DeepinvTransform(trans_homo))\n",
    "    transform_types[1].append(DeepinvTransform(trans_homo))\n",
    "\n",
    "if enable_nucleus_segmentation_based_aug:\n",
    "    print('Enabling Segmentation based augmentation.')\n",
    "    assert nucleus_channel_idx in [0,1], f'expected nucleus_channel_idx to be in [0,1] but found {nucleus_channel_idx}'\n",
    "    seg_fn = get_segmentation_fn(mean_value=mean_dict['target'].squeeze()[nucleus_channel_idx].item(), \n",
    "                                 std_value=std_dict['target'].squeeze()[nucleus_channel_idx].item())\n",
    "    seg_aug = SegmentationShuffle(mean_value=mean_dict['target'].squeeze()[nucleus_channel_idx].item(), \n",
    "                                 std_value=std_dict['target'].squeeze()[nucleus_channel_idx].item())\n",
    "    transform_types[nucleus_channel_idx].append(seg_aug)\n",
    "\n",
    "transform_all = TransformAllChannels(transform_types)\n",
    "\n",
    "# define a learnable scalar and an offset \n",
    "factor1 = torch.nn.Parameter(torch.tensor(1.0).cuda())\n",
    "offset1 = torch.nn.Parameter(torch.tensor(0.0).cuda())\n",
    "\n",
    "if optimally_scale_predictions:\n",
    "    print('Using the fixed factor and offset to scale the predictions', input_factor, input_offset)\n",
    "    factor2 = torch.tensor(input_factor)\n",
    "    offset2 = torch.tensor(input_offset)\n",
    "else:\n",
    "    factor2 = torch.nn.Parameter(torch.tensor(1.0).cuda())\n",
    "    offset2 = torch.nn.Parameter(torch.tensor(0.0).cuda())\n",
    "mixing_ratio = torch.nn.Parameter(torch.tensor(best_t_estimate).cuda())\n",
    "\n",
    "if optimaization_mode == 'twostep':\n",
    "    print('Dset shape', ft_dset[0][0].shape, ft_dset[0][1].shape, ft_val_dset[0][0].shape, ft_val_dset[0][1].shape)\n",
    "    optimization_params = [factor1, offset1]\n",
    "    finetuning_output_dict = finetune_two_forward_passes(model, ft_dset, ft_val_dset, transform_all, \n",
    "                                                        max_step_count=max_step_count_step1, \n",
    "                                                        batch_size=batch_size, \n",
    "                                                        skip_pixels=skip_pixels,\n",
    "                                                        validation_step_freq=validation_step_freq,\n",
    "                                                        psnr_evaluation=psnr_evaluation,\n",
    "                                    scalar_params_dict={'factor1':factor1, 'offset1':offset1, 'factor2':factor2, 'offset2':offset2, 'mixing_ratio':mixing_ratio},\n",
    "                                    optimization_params_dict={'lr':lr_step1, 'parameters': optimization_params},\n",
    "                                    # lookback=lookback,\n",
    "                                    k_augmentations=k_augmentations,\n",
    "                                    stats_enforcing_loss_fn=lambda x : stats_loss_func(x),\n",
    "                                    sample_mixing_ratio=enable_mixing_aug,\n",
    "                                    )\n",
    "    best_factors = finetuning_output_dict['best_factors']\n",
    "    best_offsets = finetuning_output_dict['best_offsets']\n",
    "elif optimaization_mode in ['onestep', 'norm_and_network']:\n",
    "    print('Skipping the first optimization step')\n",
    "    best_factors = [factor1.item(), factor2.item()]\n",
    "    best_offsets = [offset1.item(), offset2.item()]\n",
    "\n",
    "print(best_factors, best_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2702c6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dloader = DataLoader(ft_dset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "# for inp, tar in dloader:\n",
    "#     inp = inp\n",
    "#     tar = tar\n",
    "#     tar = (tar - data_mean['target'])/ data_std['target']\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a4994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c272a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttar, _ = seg_aug(tar[:,1].type(torch.float32))\n",
    "# ttar.isnan().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d2aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ttar, _ = transform_all(tar.type(torch.float32))\n",
    "# ttar.isnan().reshape(16,2,-1).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918c065",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "subdir = '_'.join(ckpt_dir.split('/')[-3:])\n",
    "subsubdir = f'{optimaization_mode}_B-{batch_size}_lr-{lr_step1}-{lr_step2}_kaug-{k_augmentations}_maxstep-{max_step_count_step1}-{max_step_count_step2}_ang-{aug_theta_max}_angz-{aug_theta_z_max}_shift-{aug_shift_max}'\n",
    "timesubdir = datetime.now().strftime(\"%Y%m%d\")\n",
    "direc = os.path.join(subdir,subsubdir,timesubdir)\n",
    "print(direc)\n",
    "if optimaization_mode == 'twostep':\n",
    "    fpath = os.path.join(outputdir, direc, 'after_step1.ckpt')\n",
    "    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "    if save_to_file:\n",
    "        # save the model weights.\n",
    "        torch.save(model.state_dict(), fpath)\n",
    "        print(f'Saved the model weights to {fpath}')\n",
    "        # save the factors and offsets\n",
    "        fpath = os.path.join(outputdir, direc, 'factors_offsets.txt')\n",
    "        with open(fpath, 'w') as f:\n",
    "            f.write(f'Factor1{best_factors[0]} Offset1{best_offsets[0]} Factor2{best_factors[1]} Offset2{best_offsets[1]}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0462285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.ssl_plots import plot_finetuning_loss\n",
    "if optimaization_mode == 'twostep':\n",
    "    plot_finetuning_loss(finetuning_output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54563870",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDataset:\n",
    "    def __init__(self, dset, scalar, offset):\n",
    "        self.dset = dset\n",
    "        self.scalar = scalar\n",
    "        self.offset = offset\n",
    "    \n",
    "    def train_mode(self):\n",
    "        self.dset.train_mode()\n",
    "    \n",
    "    def eval_mode(self):\n",
    "        self.dset.eval_mode()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        inp, tar = self.dset[idx]\n",
    "        return inp * self.scalar + self.offset, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95550dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_ft_dset = ScaledDataset(ft_dset, best_factors[0], best_offsets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7f68ec",
   "metadata": {},
   "source": [
    "### Loading the state from a pretrained model, along with the factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54480d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_fpath = '/group/jug/ashesh/EnsDeLyon/notebook_results/2504_D21-M3-S0-L0_14/twostep_B-128_lr-0.001-1e-05_kaug-8_maxstep-200000-1600000_ang-0_angz-0_shift-0/after_step2.ckpt'\n",
    "# print(new_fpath)\n",
    "# _ = model.load_state_dict(torch.load(new_fpath), strict=False)\n",
    "\n",
    "# scalar_fpath = os.path.join(os.path.dirname(new_fpath), 'factors_offsets.txt')\n",
    "# if os.path.exists(scalar_fpath):\n",
    "#     with open(scalar_fpath, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         tokens = lines[0].split(' ')\n",
    "#         assert 'Factor1' == tokens[0][:7]\n",
    "#         factor1 = float(tokens[0][7:])\n",
    "#         assert 'Offset1' == tokens[1][:7]\n",
    "#         offset1 = float(tokens[1][7:])\n",
    "#         assert 'Factor2' == tokens[2][:7]\n",
    "#         factor2 = float(tokens[2][7:])\n",
    "#         assert 'Offset2' == tokens[3][:7]\n",
    "#         offset2 = float(tokens[3][7:])\n",
    "#         best_factors = [factor1, factor2]\n",
    "#         best_offsets = [offset1, offset2]\n",
    "    \n",
    "# scaled_val_dset = ScaledDataset(val_dset, best_factors[0], best_offsets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9493e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dset.eval_mode()\n",
    "set_image_size_for_evaluation()\n",
    "print('Dset shape', ft_dset[0][0].shape, ft_dset[0][1].shape, ft_val_dset[0][0].shape, ft_val_dset[0][1].shape)\n",
    "pred_tiled, rec_loss, logvar_tiled, patch_psnr_tuple, pred_std_tiled, _ = get_dset_predictions(model, scaled_ft_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=mmse_count,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4065edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stitch_predictions(pred_tiled,ft_dset)\n",
    "pred_unnorm = get_unnorm_pred(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fef646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_unnorm = pred_unnorm[0]\n",
    "# pred = pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff58347",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_one_sample(normalize_channels(tar_unnorm), pred, inp_unnorm, config, best_t_estimate, img_idx=img_idx, hs=hs, ws=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4388ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_ch0, psnr_ch1 = get_psnr_numbers(tar_unnorm, pred_unnorm)\n",
    "estimated_input = get_input_normalized(pred,best_t_estimate, channel_pos='last')\n",
    "mse_input_str = 'MSE-input ' +' '.join([f'{np.mean((gt_tmp - pred_tmp)**2):.2f}' for gt_tmp,pred_tmp in zip(actual_input, estimated_input)])\n",
    "print(f'F1:{best_factors[0]:.2f} Off1:{best_offsets[0]:.2f} F2:{best_factors[1]:.2f} Off2:{best_offsets[1]:.2f}\\t\\t PSNR: {psnr_ch0[0]:.2f} {psnr_ch1[0]:.2f} {mse_input_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6429bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_image_size_for_finetuning()\n",
    "# reload()\n",
    "# batch_size = 128\n",
    "print(max_step_count_step2)\n",
    "# best_factors[0], best_offsets[0]\n",
    "if optimaization_mode == 'norm_and_network':\n",
    "    scalar_param_dict = {'factor1':factor1, 'offset1':offset1, 'factor2':factor2, 'offset2':offset2, 'mixing_ratio':mixing_ratio}\n",
    "    finetuning_params = [factor1, offset1] + list(model.parameters())\n",
    "else:\n",
    "    finetuning_params = model.parameters()\n",
    "    scalar_param_dict = {'factor1':torch.Tensor([best_factors[0]]).cuda(), \n",
    "                                                    'offset1':torch.Tensor([best_offsets[0]]).cuda(), \n",
    "                                                    'factor2':torch.Tensor([best_factors[1]]).cuda(), \n",
    "                                                    'offset2':torch.Tensor([best_offsets[1]]).cuda(), \n",
    "                                                    'mixing_ratio':mixing_ratio}\n",
    "\n",
    "print('Dset shape', ft_dset[0][0].shape, ft_dset[0][1].shape, ft_val_dset[0][0].shape, ft_val_dset[0][1].shape)\n",
    "finetuning_output_dict = finetune_two_forward_passes(model, ft_dset, ft_val_dset, transform_all, max_step_count=max_step_count_step2, \n",
    "                                                    batch_size=batch_size, skip_pixels=skip_pixels,\n",
    "                                                    scalar_params_dict=scalar_param_dict,\n",
    "                                                    validation_step_freq=validation_step_freq,\n",
    "                                                    optimization_params_dict={'lr':lr_step2, \n",
    "\n",
    "                                                                            #   'parameters': model.first_bottom_up.parameters()\n",
    "                                                                            'parameters': finetuning_params\n",
    "                                                                            },\n",
    "                                                    # lookback=lookback,\n",
    "                                                                k_augmentations=k_augmentations,\n",
    "                                                                psnr_evaluation=psnr_evaluation,\n",
    "                                                                stats_enforcing_loss_fn=lambda x : stats_loss_func(x),\n",
    "                                                                sample_mixing_ratio=enable_mixing_aug,)\n",
    "\n",
    "best_factors = finetuning_output_dict['best_factors']\n",
    "best_offsets = finetuning_output_dict['best_offsets']\n",
    "scaled_ft_dset = ScaledDataset(ft_dset, best_factors[0], best_offsets[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e72898",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best factors: ', best_factors, best_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43adb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.analysis.ssl_plots import plot_finetuning_loss\n",
    "plot_finetuning_loss(finetuning_output_dict, loss_rolling=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c742ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_dset.eval_mode()\n",
    "set_image_size_for_evaluation()\n",
    "print('Dset shape', ft_dset[0][0].shape, ft_dset[0][1].shape, ft_val_dset[0][0].shape, ft_val_dset[0][1].shape)\n",
    "pred_tiled, rec_loss, logvar_tiled, patch_psnr_tuple, pred_std_tiled, _ = get_dset_predictions(\n",
    "                                              model, \n",
    "                                              scaled_ft_dset,\n",
    "                                              batch_size,\n",
    "                                              num_workers=num_workers,\n",
    "                                              mmse_count=mmse_count,\n",
    "                                              model_type = config.model.model_type,\n",
    "                                              )\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312917f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stitch_predictions(pred_tiled,ft_dset)\n",
    "pred_unnorm = get_unnorm_pred(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf71a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(10,5),ncols=2)\n",
    "ax[0].imshow(pred[0][...,0].squeeze())\n",
    "ax[1].imshow(pred[0][...,1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dd349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ = plot_one_sample(normalize_channels(tar_unnorm), pred, inp_unnorm, config, best_t_estimate, img_idx=img_idx, hs=hs, ws=ws)\n",
    "_ = plot_one_sample(normalize_channels(tar_unnorm), pred, inp_unnorm, config, best_t_estimate, img_idx=img_idx, hs=hs, ws=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871d9b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_ch0, psnr_ch1 = get_psnr_numbers(tar_unnorm, pred_unnorm)\n",
    "estimated_input = get_input_normalized(pred,best_t_estimate, channel_pos='last')\n",
    "mse_input_str = 'MSE-input ' +' '.join([f'{np.mean((gt_tmp - pred_tmp)**2):.2f}' for gt_tmp,pred_tmp in zip(actual_input, estimated_input)])\n",
    "print(f'F1:{best_factors[0]:.2f} Off1:{best_offsets[0]:.2f} F2:{best_factors[1]:.2f} Off2:{best_offsets[1]:.2f}\\t\\t PSNR: {psnr_ch0[0]:.2f} {psnr_ch1[0]:.2f} {mse_input_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_to_file:\n",
    "    fpath = os.path.join(outputdir, direc, 'after_step2.ckpt')\n",
    "    os.makedirs(os.path.dirname(fpath), exist_ok=True)\n",
    "    # save the model weights.\n",
    "    torch.save(model.state_dict(), fpath)\n",
    "    print(f'Saved the model weights to {fpath}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03574c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_img_raw = val_dset.dsets[0]._data[0,400:656,256:512,0]\n",
    "# plt.imshow(test_img_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db74f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from finetunesplit.aug_patch_shuffle import GridShuffle\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# trans = GridShuffle(256, 128)\n",
    "# test_img = torch.Tensor(test_img_raw[None, None]*1.0)\n",
    "# pred, ddict = trans(test_img)\n",
    "# trans.set_params(ddict['inverse']['shuffle_order'])\n",
    "# inv_pred, _ = trans(pred)\n",
    "# _, ax = plt.subplots(figsize=(15,5),ncols=3)\n",
    "# ax[0].imshow(test_img[0,0])\n",
    "# ax[1].imshow(pred[0,0])\n",
    "# ax[2].imshow(inv_pred[0,0])\n",
    "# ax[0].set_title('Original')\n",
    "# ax[1].set_title('Shuffled')\n",
    "# ax[2].set_title('Inverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62e6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
