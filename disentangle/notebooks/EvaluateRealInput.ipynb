{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edd04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19844352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e7d7c01",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this notebook, we will show how to evaluate the performance of a model on a test or validation set. \n",
    "This notebook assumes that the network has already been trained and saved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2b7f917",
   "metadata": {},
   "source": [
    "# Input\n",
    "1. data_dir: directory of the data. The datafile should be present in the data_dir.\n",
    "2. ckpt_dir: directory of the checkpoint. The checkpoint file and config should be present in the ckpt_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ec4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "import ml_collections\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from disentangle.training import create_dataset, create_model\n",
    "import matplotlib.pyplot as plt\n",
    "from disentangle.core.loss_type import LossType\n",
    "from disentangle.config_utils import load_config\n",
    "from disentangle.analysis.lvae_utils import get_img_from_forward_output\n",
    "from disentangle.analysis.plot_utils import clean_ax\n",
    "from disentangle.core.data_type import DataType\n",
    "from disentangle.core.psnr import PSNR\n",
    "from disentangle.analysis.plot_utils import get_k_largest_indices,plot_imgs_from_idx\n",
    "from disentangle.core.psnr import PSNR, RangeInvariantPsnr\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.analysis.stitch_prediction import stitch_predictions\n",
    "from disentangle.analysis.mmse_prediction import get_dset_predictions\n",
    "\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b237569",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ckpt_dir = '/group/jug/ashesh/training/disentangle/2505/D36-M3-S0-L8/3'\n",
    "data_dir = '/group/jug/ashesh/data/diffsplit_HT_LIF/500ms/Ch_B-Ch_D-Ch_BD/'\n",
    "# data_dir = '/group/jug/ashesh/data/diffsplit_HT_T24/'\n",
    "\n",
    "mmse_count = 50\n",
    "image_size_for_grid_centers = 32\n",
    "custom_image_size = 64\n",
    "\n",
    "# MIXING_WEIGHT = 0.3\n",
    "# MAX_VAL = 1993 # This is just for HAGEN dataset !!! \n",
    "\n",
    "batch_size = 32\n",
    "num_workers = 4\n",
    "use_deterministic_grid = None\n",
    "input_channel_idx = 3\n",
    "threshold = None # 0.02\n",
    "compute_kl_loss = False\n",
    "evaluate_train = False# inspect training performance\n",
    "eval_datasplit_type = DataSplitType.Test\n",
    "val_repeat_factor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3360bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_checkpoint(ckpt_dir):\n",
    "    output = []\n",
    "    for filename in glob.glob(ckpt_dir + \"/*_best.ckpt\"):\n",
    "        output.append(filename)\n",
    "    assert len(output) == 1, '\\n'.join(output)\n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.model_type import ModelType\n",
    "config = load_config(ckpt_dir)\n",
    "config = ml_collections.ConfigDict(config)\n",
    "old_image_size = None\n",
    "with config.unlocked():\n",
    "    # This was added to make sure we get the real input.\n",
    "    config.data.keep_real_input = True\n",
    "    # if config.data.data_type in [DataType.TavernaSox2GolgiV2, DataType.indiSplit_HTT24]:\n",
    "    config.data.num_channels = 3\n",
    "        \n",
    "    if input_channel_idx is not None:\n",
    "        config.data.input_channel_idx = input_channel_idx\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if 'test_fraction' not in config.training:\n",
    "        config.training.test_fraction =0.0\n",
    "        \n",
    "    if 'datadir' not in config:\n",
    "        config.datadir = ''\n",
    "    if 'encoder' not in config.model:\n",
    "        config.model.encoder = ml_collections.ConfigDict()\n",
    "        assert 'decoder' not in config.model\n",
    "        config.model.decoder = ml_collections.ConfigDict()\n",
    "    \n",
    "        config.model.encoder.dropout = config.model.dropout\n",
    "        config.model.decoder.dropout = config.model.dropout\n",
    "        config.model.encoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.decoder.blocks_per_layer = config.model.blocks_per_layer\n",
    "        config.model.encoder.n_filters = config.model.n_filters\n",
    "        config.model.decoder.n_filters = config.model.n_filters\n",
    "        \n",
    "    if 'multiscale_retain_spatial_dims' not in config.model.decoder:\n",
    "        config.model.decoder.multiscale_retain_spatial_dims = False\n",
    "        \n",
    "    if 'res_block_kernel' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_kernel = 3\n",
    "        assert 'res_block_kernel' not in config.model.decoder\n",
    "        config.model.decoder.res_block_kernel = 3\n",
    "    \n",
    "    if 'res_block_skip_padding' not in config.model.encoder:\n",
    "        config.model.encoder.res_block_skip_padding = False\n",
    "        assert 'res_block_skip_padding' not in config.model.decoder\n",
    "        config.model.decoder.res_block_skip_padding = False\n",
    "    \n",
    "    if config.data.data_type == DataType.CustomSinosoid:\n",
    "        if 'max_vshift_factor' not in config.data:\n",
    "            config.data.max_vshift_factor = config.data.max_shift_factor\n",
    "            config.data.max_hshift_factor = 0\n",
    "        if 'encourage_non_overlap_single_channel' not in config.data:\n",
    "            config.data.encourage_non_overlap_single_channel = False\n",
    "            \n",
    "    if 'skip_bottom_layers_count' in config.model:\n",
    "        config.model.skip_bottom_layers_count = 0\n",
    "        \n",
    "    if 'logvar_lowerbound' not in config.model:\n",
    "        config.model.logvar_lowerbound = None\n",
    "    if 'train_aug_rotate' not in config.data:\n",
    "        config.data.train_aug_rotate = False\n",
    "    if 'multiscale_lowres_separate_branch' not in config.model:\n",
    "        config.model.multiscale_lowres_separate_branch = False\n",
    "    if 'multiscale_retain_spatial_dims' not in config.model:\n",
    "        config.model.multiscale_retain_spatial_dims = False\n",
    "    config.data.train_aug_rotate=False\n",
    "    \n",
    "    if 'randomized_channels' not in config.data:\n",
    "        config.data.randomized_channels = False\n",
    "        \n",
    "    if 'predict_logvar' not in config.model:\n",
    "        config.model.predict_logvar=None\n",
    "    \n",
    "    if 'batchnorm' in config.model and 'batchnorm' not in config.model.encoder:\n",
    "        assert 'batchnorm' not in config.model.decoder\n",
    "        config.model.decoder.batchnorm = config.model.batchnorm\n",
    "        config.model.encoder.batchnorm = config.model.batchnorm\n",
    "    if 'conv2d_bias' not in config.model.decoder:\n",
    "        config.model.decoder.conv2d_bias = True\n",
    "        \n",
    "    \n",
    "    if custom_image_size is not None:\n",
    "        old_image_size = config.data.image_size\n",
    "        config.data.image_size = custom_image_size\n",
    "    if image_size_for_grid_centers is not None:\n",
    "        old_grid_size = config.data.get('grid_size', \"grid_size not present\")\n",
    "        config.data.grid_size = image_size_for_grid_centers\n",
    "        config.data.val_grid_size = image_size_for_grid_centers\n",
    "\n",
    "    if use_deterministic_grid is not None:\n",
    "        config.data.deterministic_grid = use_deterministic_grid\n",
    "    if threshold is not None:\n",
    "        config.data.threshold = threshold\n",
    "    if val_repeat_factor is not None:\n",
    "        config.training.val_repeat_factor = val_repeat_factor\n",
    "    config.model.mode_pred = not compute_kl_loss\n",
    "\n",
    "    config.model.skip_nboundary_pixels_from_loss = None\n",
    "    if config.model.model_type == ModelType.UNet and 'n_levels' not in config.model:\n",
    "        config.model.n_levels = 4\n",
    "    \n",
    "    if config.model.model_type == ModelType.UNet and 'init_channel_count' not in config.model:\n",
    "        config.model.init_channel_count = 64\n",
    "    \n",
    "    if 'skip_receptive_field_loss_tokens' not in config.loss:\n",
    "        config.loss.skip_receptive_field_loss_tokens = []\n",
    "    \n",
    "    if 'lowres_merge_type' not in config.model.encoder:\n",
    "        config.model.encoder.lowres_merge_type = 0\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53db9fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are doing the clipping in the other way.\n",
    "config.data.clip_percentile = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71cfc106",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dacc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from disentangle.data_loader.multi_channel_determ_tiff_dloader import MultiChDeterministicTiffDloader\n",
    "# from disentangle.data_loader.lc_tiff_dloader import MultiScaleTiffDloader\n",
    "from disentangle.core.data_split_type import DataSplitType\n",
    "from disentangle.data_loader.patch_index_manager import TilingMode\n",
    "from disentangle.data_loader.lc_multich_dloader import LCMultiChDloader\n",
    "from disentangle.data_loader.vanilla_dloader import MultiChDloader\n",
    "\n",
    "class TiffDloaderRealInput(MultiChDloader):\n",
    "    \"\"\"\n",
    "    Now, we have just the inputs, and no targets. \n",
    "    \"\"\"\n",
    "    def load_data(self, data_config, datasplit_type, val_fraction=None, test_fraction=None, allow_generation=None):\n",
    "        super().load_data(data_config, datasplit_type, val_fraction=val_fraction, test_fraction=test_fraction, allow_generation=allow_generation)\n",
    "        self._data_original = self._data\n",
    "        new_data = self._data.copy()\n",
    "        assert new_data.shape[-1] == 3, 'Expecting 3 channels with the last channel being the input'\n",
    "        print('Setting the data to the last channel')\n",
    "        self._data = new_data[...,2:3]\n",
    "        self._data = np.repeat(self._data, 2, axis=-1)\n",
    "\n",
    "class MultiScaleTiffDloaderRealInput(LCMultiChDloader):\n",
    "    \"\"\"\n",
    "    Now, we have just the inputs, and no targets. \n",
    "    \"\"\"\n",
    "    def load_data(self, data_config, datasplit_type, val_fraction=None, test_fraction=None, allow_generation=None):\n",
    "        super().load_data(data_config, datasplit_type, val_fraction=val_fraction, test_fraction=test_fraction, allow_generation=allow_generation)\n",
    "        self._data_original = self._data\n",
    "        new_data = self._data.copy()\n",
    "        assert new_data.shape[-1] == 3, 'Expecting 3 channels with the last channel being the input, but got {}'.format(new_data.shape)\n",
    "        print('Setting the data to the last channel')\n",
    "        self._data = new_data[...,2:3]\n",
    "        self._data = np.repeat(self._data, 2, axis=-1)\n",
    "\n",
    "padding_kwargs = {\n",
    "    'mode':config.data.get('padding_mode','constant'),\n",
    "}\n",
    "\n",
    "if padding_kwargs['mode'] == 'constant':\n",
    "    padding_kwargs['constant_values'] = config.data.get('padding_value',0)\n",
    "\n",
    "dloader_kwargs = {'overlapping_padding_kwargs':padding_kwargs}\n",
    "if 'multiscale_lowres_count' in config.data and config.data.multiscale_lowres_count is not None:\n",
    "    data_class = MultiScaleTiffDloaderRealInput\n",
    "    dloader_kwargs['num_scales'] = config.data.multiscale_lowres_count\n",
    "    dloader_kwargs['padding_kwargs'] = padding_kwargs\n",
    "else:\n",
    "    # raise Exception('Not implemented')\n",
    "    data_class = TiffDloaderRealInput\n",
    "\n",
    "# if config.data.data_type in [DataType.CustomSinosoid, DataType.CustomSinosoidThreeCurve, \n",
    "#                              DataType.SeparateTiffData, DataType.HTLIF24, DataType.TavernaSox2GolgiV2,\n",
    "#                             ]:\n",
    "    # datapath = data_dir\n",
    "if config.data.data_type == DataType.OptiMEM100_014:\n",
    "    datapath = os.path.join(data_dir, 'OptiMEM100x014.tif')\n",
    "else:\n",
    "    datapath = data_dir\n",
    "\n",
    "normalized_input = config.data.normalized_input\n",
    "use_one_mu_std = config.data.use_one_mu_std\n",
    "train_aug_rotate = config.data.train_aug_rotate\n",
    "enable_random_cropping = False\n",
    "grid_alignment = TilingMode.ShiftBoundary\n",
    "print(data_class)\n",
    "\n",
    "train_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=DataSplitType.Train,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=train_aug_rotate,\n",
    "                enable_random_cropping=enable_random_cropping,\n",
    "                # grid_alignment=grid_alignment,\n",
    "                **dloader_kwargs)\n",
    "import gc\n",
    "gc.collect()\n",
    "max_val = train_dset.get_max_val()\n",
    "\n",
    "val_dset = data_class(\n",
    "                config.data,\n",
    "                datapath,\n",
    "                datasplit_type=eval_datasplit_type,\n",
    "                val_fraction=config.training.val_fraction,\n",
    "                test_fraction=config.training.test_fraction,\n",
    "                normalized_input=normalized_input,\n",
    "                use_one_mu_std=use_one_mu_std,\n",
    "                enable_rotation_aug=False,  # No rotation aug on validation\n",
    "                enable_random_cropping=False,\n",
    "                # No random cropping on validation. Validation is evaluated on determistic grids\n",
    "                # grid_alignment=grid_alignment,\n",
    "                max_val=max_val,\n",
    "                **dloader_kwargs\n",
    "                \n",
    "            )\n",
    "\n",
    "# For normalizing, we should be using the training data's mean and std.\n",
    "mean_val, std_val = train_dset.compute_mean_std()\n",
    "train_dset.set_mean_std(mean_val, std_val)\n",
    "val_dset.set_mean_std(mean_val, std_val)\n",
    "\n",
    "\n",
    "if evaluate_train:\n",
    "    val_dset = train_dset\n",
    "data_mean, data_std = train_dset.get_mean_std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511d4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dset._data.max(), train_dset._data.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c69e23b",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a995b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with config.unlocked():\n",
    "    if old_image_size is not None:\n",
    "        config.data.image_size = old_image_size\n",
    "\n",
    "if config.data.target_separate_normalization is True:\n",
    "    mean_fr_model, std_fr_model = train_dset.compute_individual_mean_std()\n",
    "else:\n",
    "    mean_fr_model, std_fr_model = train_dset.get_mean_std()\n",
    "\n",
    "\n",
    "model = create_model(config, {'target':mean_fr_model},{'target':std_fr_model})\n",
    "\n",
    "ckpt_fpath = get_best_checkpoint(ckpt_dir)\n",
    "checkpoint = torch.load(ckpt_fpath)\n",
    "\n",
    "_ = model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()\n",
    "_= model.cuda()\n",
    "\n",
    "model.set_params_to_same_device_as(torch.Tensor(1).cuda())\n",
    "\n",
    "print('Loading from epoch', checkpoint['epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2916626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Model has {count_parameters(model)/1000_000:.3f}M parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d5fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.multiscale_lowres_count is not None and custom_image_size is not None:\n",
    "    model.reset_for_different_output_size(custom_image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59475e42",
   "metadata": {},
   "source": [
    "## Mean/Std as present in usplit. Just run Evaluate.ipynb for this config and set it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed50df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset.get_mean_std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba12d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.data_type == DataType.SeparateTiffData:\n",
    "    mean_val_usplit = np.array([[[[237.37582]],[[237.37582]]]])\n",
    "    std_val_usplit = np.array([[[[299.02316]],[[299.02316]]]])\n",
    "elif config.data.data_type in [DataType.indiSplit_HTLIF24,DataType.indiSplit_HTT24, DataType.TavernaSox2GolgiV2]:\n",
    "    mean_val_usplit, std_val_usplit = train_dset.get_mean_std()\n",
    "else:\n",
    "    raise NotImplementedError(config.data.data_type)\n",
    "val_dset.set_mean_std(mean_val_usplit, std_val_usplit)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d23d6c0",
   "metadata": {},
   "source": [
    "## Looking at the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05be428",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(len(val_dset))\n",
    "inp_tmp, tar_tmp, *_ = val_dset[idx]\n",
    "if inp_tmp.ndim == 2:\n",
    "    inp_tmp = inp_tmp[None,...]\n",
    "\n",
    "ncols = max(len(inp_tmp),3)\n",
    "nrows = 2\n",
    "_,ax = plt.subplots(figsize=(4*ncols,4*nrows),ncols=ncols,nrows=nrows)\n",
    "for i in range(len(inp_tmp)):\n",
    "    ax[0,i].imshow(inp_tmp[i])\n",
    "\n",
    "\n",
    "ax[1,0].imshow(tar_tmp[0])\n",
    "ax[1,1].imshow(tar_tmp[1])\n",
    "\n",
    "ax[0,0].set_ylabel('Input')\n",
    "ax[1,0].set_ylabel('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac092b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_tiled, rec_loss, logvar, patch_psnr_tuple,predictions_std = get_dset_predictions(model, val_dset,batch_size,\n",
    "                                               num_workers=num_workers,\n",
    "                                               mmse_count=mmse_count,\n",
    "                                                model_type = config.model.model_type,\n",
    "                                              )\n",
    "\n",
    "tmp = np.round([x.item() for x in patch_psnr_tuple],2)\n",
    "print('Patch wise PSNR, as computed during training', tmp,np.mean(tmp) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b693a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = np.where(logvar.squeeze() < -6)[0]\n",
    "if len(idx_list) > 0:\n",
    "    plt.imshow(val_dset[idx_list[0]][1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75b35f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stitch_predictions(pred_tiled,val_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ad25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ignored_pixels():\n",
    "    ignored_pixels = 1\n",
    "    while(pred[0,-ignored_pixels:,-ignored_pixels:,].std() ==0):\n",
    "        ignored_pixels+=1\n",
    "    ignored_pixels-=1\n",
    "    print(f'In {pred.shape}, last {ignored_pixels} many rows and columns are all zero.')\n",
    "    return ignored_pixels\n",
    "\n",
    "ignored_pixels_in_data = print_ignored_pixels()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8474735",
   "metadata": {},
   "source": [
    "## Ignore the pixels which are present in the last few rows and columns. \n",
    "1. They don't come in the batches. So, in prediction, they are simply zeros. So they are being are ignored right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2db09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_pixels_in_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadedfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_last_pixels_count = 32 if config.data.data_type == DataType.OptiMEM100_014 else ignored_pixels_in_data\n",
    "\n",
    "assert ignored_pixels_in_data <= ignore_last_pixels_count, f'Set ignore_last_pixels_count={ignored_pixels_in_data}'\n",
    "print(ignore_last_pixels_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226fed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar = val_dset._data_original[...,:-1]\n",
    "if config.data.data_type == DataType.SeparateTiffData:\n",
    "    tar[ tar > MAX_VAL] = MAX_VAL\n",
    "\n",
    "def ignore_pixels(arr):\n",
    "    if ignore_last_pixels_count:\n",
    "        arr = arr[:,:-ignore_last_pixels_count,:-ignore_last_pixels_count]\n",
    "    return arr\n",
    "\n",
    "pred = ignore_pixels(pred)\n",
    "tar = ignore_pixels(tar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "def _avg_psnr(target, prediction, psnr_fn):\n",
    "    output = np.mean([psnr_fn(target[i:i + 1], prediction[i:i + 1]).item() for i in range(len(prediction))])\n",
    "    return round(output, 2)\n",
    "\n",
    "\n",
    "def avg_range_inv_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, RangeInvariantPsnr)\n",
    "\n",
    "\n",
    "def avg_psnr(target, prediction):\n",
    "    return _avg_psnr(target, prediction, PSNR)\n",
    "\n",
    "\n",
    "def compute_masked_psnr(mask, tar1, tar2, pred1, pred2):\n",
    "    mask = mask.astype(bool)\n",
    "    mask = mask[..., 0]\n",
    "    tmp_tar1 = tar1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_pred1 = pred1[mask].reshape((len(tar1), -1, 1))\n",
    "    tmp_tar2 = tar2[mask].reshape((len(tar2), -1, 1))\n",
    "    tmp_pred2 = pred2[mask].reshape((len(tar2), -1, 1))\n",
    "    psnr1 = avg_range_inv_psnr(tmp_tar1, tmp_pred1)\n",
    "    psnr2 = avg_range_inv_psnr(tmp_tar2, tmp_pred2)\n",
    "    return psnr1, psnr2\n",
    "\n",
    "def avg_ssim(target, prediction):\n",
    "    ssim = [structural_similarity(target[i],prediction[i], data_range=(target[i].max() - target[i].min())) for i in range(len(target))]\n",
    "    return np.mean(ssim),np.std(ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf6ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.data.data_type == DataType.SeparateTiffData:\n",
    "    sep_mean = np.array([[[[237.37582]],[[237.37582]]]])\n",
    "    sep_std = np.array([[[[299.02316]],[[299.02316]]]])\n",
    "elif config.data.data_type in [DataType.indiSplit_HTLIF24]:\n",
    "    sep_mean = np.array([[[[411.70557]],[[349.42032]]]])\n",
    "    sep_std = np.array([[[[630.8665]],[[327.70892]]]])\n",
    "elif config.data.data_type in [DataType.TavernaSox2GolgiV2, DataType.indiSplit_HTT24]:\n",
    "    sep_mean = np.array([[[[247.25721281]],[[333.98826175]]]])\n",
    "    sep_std = np.array([[[[135.71459324]],[[257.38415285]]]])\n",
    "elif config.data.data_type in [DataType.CosemHela]:\n",
    "    sep_mean, sep_std = train_dset.get_mean_std()\n",
    "else:\n",
    "    raise NotImplementedError(config.data.data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7311e08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sep_mean, sep_std = model.data_mean, model.data_std\n",
    "if isinstance(sep_mean, dict):\n",
    "    sep_mean = sep_mean['target']\n",
    "    sep_std = sep_std['target']\n",
    "    \n",
    "sep_mean = sep_mean.squeeze()[None,None,None]\n",
    "sep_std = sep_std.squeeze()[None,None,None]\n",
    "\n",
    "# tar_normalized = (tar - sep_mean)/sep_std\n",
    "# tar1 =tar_normalized[...,0]\n",
    "# tar2 =tar_normalized[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24708c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(12,12),ncols=2,nrows=2)\n",
    "idx = np.random.randint(len(pred))\n",
    "print(idx)\n",
    "ax[0,0].imshow(pred[idx,:,:,0])\n",
    "ax[0,1].imshow(pred[idx,:,:,1])\n",
    "ax[1,0].imshow(tar[idx,:,:,0])\n",
    "ax[1,1].imshow(tar[idx,:,:,1])\n",
    "\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fc1983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1, pred2 = pred[...,0].astype(np.float32), pred[...,1].astype(np.float32)\n",
    "# rmse1 =np.sqrt(((pred1 - tar1)**2).reshape(len(pred1),-1).mean(axis=1))\n",
    "# rmse2 =np.sqrt(((pred2 - tar2)**2).reshape(len(pred2),-1).mean(axis=1)) \n",
    "\n",
    "# rmse = (rmse1 + rmse2)/2\n",
    "# rmse = np.round(rmse,3)\n",
    "# psnr1 = avg_psnr(tar1, pred1) \n",
    "# psnr2 = avg_psnr(tar2, pred2)\n",
    "# rinv_psnr1 = avg_range_inv_psnr(tar1, pred1)\n",
    "# rinv_psnr2 = avg_range_inv_psnr(tar2, pred2)\n",
    "# ssim1_mean, ssim1_std = avg_ssim(tar1, pred1)\n",
    "# ssim2_mean, ssim2_std = avg_ssim(tar2, pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87868b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f' {DataSplitType.name(eval_datasplit_type)}_P{custom_image_size}_G{image_size_for_grid_centers}_M{mmse_count}_Sk{ignore_last_pixels_count}')\n",
    "# print('Rec Loss',np.round(rec_loss.mean(),3) )\n",
    "# print('RMSE', np.mean(rmse1).round(3), np.mean(rmse2).round(3), np.mean(rmse).round(3))\n",
    "# print('[Paper] RangeInv PSNR',rinv_psnr1, rinv_psnr2 )\n",
    "# print('[Paper] SSIM',round(ssim1_mean,3), round(ssim2_mean,3),'Â±',round((ssim1_std + ssim2_std)/2,4))\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdf085",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sep_mean.shape, sep_std.shape)\n",
    "pred_unnormalized = pred * sep_std + sep_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb9e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from disentangle.core.metrics import compute_stats\n",
    "stats_dict = compute_stats(tar, pred_unnormalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee92e4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread, imsave\n",
    "def save_tiff(path, data):\n",
    "    imsave(path, data, plugin='tifffile')\n",
    "\n",
    "pred_unnormalized.shape\n",
    "param_str = f\"MMSE-{mmse_count}_RealInput\"\n",
    "ckpt_str = 'MicroSplit_' + '_'.join(ckpt_dir.split('/')[-3:])\n",
    "result_dir = os.path.join('/group/jug/ashesh/indiSplit/', 'prediction_baselines',ckpt_str + '_' + param_str)\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "print(result_dir)\n",
    "\n",
    "\n",
    "\n",
    "save_tiff(os.path.join(result_dir,'pred.tif'), pred_unnormalized)\n",
    "# save_tiff(os.path.join(result_dir,'dim_pred.tif'), pred_dim_pred_stitched)\n",
    "# pred_bt_removed_stitched.shape, pred_bt_removed_stitched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de41db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "semanticunmix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
